[{
    "title": "A year in the Ubuntu Community Council",
    "date": "",
    "description": "",
    "body": "Let me tell you a secret. When I was elected to the Ubuntu Community Council in the fall of 2022, I had no idea what to expect. I was excited to start working, but I had no idea what I would be working on.\nI\u0026rsquo;m clearly not the only person with this issue, given the first question of the Ubuntu Community Council Q\u0026amp;A session at the summit was \u0026ldquo;What does the council actually do?\u0026rdquo;\nThe truth is that it\u0026rsquo;s complicated. I\u0026rsquo;m writing this blog to explain some of the things we\u0026rsquo;ve been doing over the past year, as a way to answer this question by giving examples.\nThe first part of the answer to this question is easy: the community council is responsible for enforcing the Ubuntu Code of Conduct and are the final arbiters of any dispute. We\u0026rsquo;re also the highest governance body in the Ubuntu Community and run elections to delegate that power to others. If you\u0026rsquo;re an Ubuntu Member, you\u0026rsquo;ve probably received a few emails from me, asking you to once again vote on various leadership bodies.\nBut this isn\u0026rsquo;t the entire answer, in my opinion. The Ubuntu community is large and distributed across hundreds of projects under our umbrella. Over the past year, I\u0026rsquo;ve met so many amazing people in our community with great ideas and a lot of excitement to work on them. Many talk about success stories: how they built a new flavor out of nothing, or how they created an amazing community on Discord. But many also talk about failures: how they\u0026rsquo;re trying to build a community around their software but don\u0026rsquo;t know where to start, or how they\u0026rsquo;re trying to get their community officially recognized and supported, but don\u0026rsquo;t know where to ask. The Ubuntu project is an incredibly complex system and it\u0026rsquo;s easy to get lost.\nThis is where I think the community council can help. We should give guidance and support to communities and individuals in need, so they don\u0026rsquo;t get lost and can keep growing. This can come in many forms.\n Connecting the right people to give an initiative a speedboost. Encouraging people with great ideas to work them out. Setting up formal governance and delegating authority to initiatives that improve our community. Praising people that do good work and encouraging them to get recognized and grow into leaders.  The community council is well-positioned to do this work. We\u0026rsquo;re well-connected to both Canonical and senior community members. We have a lot of collective experience with how to get things done in this community, and the project looks to us as a unifying authority.\nSo what have we been doing exactly?\nKickstarting Ubuntu Matrix Governance Ubuntu has been slowly testing out Matrix as a modern chat platform. It started as an informal Ubuntu space on a personal homeserver, and slowly grew in size as more people started getting excited. However, in November 2023, the team working in this bumped up against an issue. Canonical had just setup a public testing homeserver for the Ubuntu community, but it received a lot of traction very quickly. Although the initial plan was to wipe the homeserver after a few months of use, suddenly entire communities started switching and using it full-time.\nWhile this was a good issue to have, it was an issue nonetheless, because the project wasn\u0026rsquo;t actually ready for this influx of users. A lot of things needed to be decided, like how to do moderation, what to bridge, who gets what permissions. However, it wasn\u0026rsquo;t actually clear who could make these decisions, because of how informal it started.\nSo to make sure this project could move forward at the speed that the community was adopting it, we decided to form the \u0026ldquo;temporary Matrix Council\u0026rdquo;, comprised of representatives from the Matrix operators, the Community Council, the IRC Council and Canonical\u0026rsquo;s community team. With a fixed term, their goal is to lead the Ubuntu Matrix project until formal elections in March. They\u0026rsquo;ve done a great job so far, you can read all about the accomplishments in the announcement of our homeserver going live.\nSupporting Local Communities While Ubuntu is an international and often digital community, we also have the concept of \u0026ldquo;Local Communities (LoCos)\u0026rdquo; that bring together community members based on geography and language. This makes it much easier to get in-person connections to our project without having to travel to the other side of the world. It also helps the diversity of our project by providing a shared voice for people with similar backgrounds. Moreover, while English is the default language of our project, Local Communities allow people to connect using other languages. As a result, they\u0026rsquo;re often a driving force of the translation efforts that make Ubuntu available to a much wider audience.\nAs Bhavanishankar Ravindra pointed out last year, one of the issues holding these communities back is the lack of guidance and cross-LoCo collaboration. To solve this, he suggested to resurrect the LoCo Council. Previous attempts have failed to gather steam, but given the incredible interest, this seemed like the right time to try again. Monica Ayhens-Madon was able to setup a community council meeting where we decided to reboot the LoCo Council with new elections. After a lot of advertising at the Ubuntu Summit, we were able to get a team of nominees together and setup elections, resulting in the new LoCo Council, serving a term of 2 years.\nAsk the Community Council anything José had the wonderful idea of organizing a public Q\u0026amp;A at the Ubuntu Summit. Technical difficulties aside, this was a great session with wonderful feedback from the community.\n  Documenting how to take conflict to the Community Council While Code of Conduct enforcement is an important part of our job, it\u0026rsquo;s not something we can publicly talk about. It\u0026rsquo;s important to ensure the privacy and safety of everyone involved, so we do not communicate publicly about investigations and actions. However, what I can talk about are some small documentation tweaks to make it more clear how people can report CoC violations and escalate matters to the community council.\n We now have a page explaining how to take conflict to the Community Council, and we added a statement to the end of the Ubuntu Code of Conduct explaining where members can report violations.  Saying thanks The community council often plays only a limited role in these efforts. While we can guide and connect people, and give authority to iniatives, the real work is being done by the incredibly community that is driving this project. The LoCo council wouldn\u0026rsquo;t be here without the work of dozens of LoCo leaders to revive the initiative. The Ubuntu Matrix project started long before we got involved, and is being carried by an amazing group of volunteers. Finally, the Canonical Community Team is always there to support us when we need it.\nLooking forward There are many more things to do for us in 2024 and beyond. These are some of the initiatives I want to support in the future.\n The temporary Matrix Council is transitioning into an elected governance body. A number of different teams and volunteers are working to document more of Ubuntu\u0026rsquo;s processes to reduce the number of people getting lost and frustrated when trying to contribute. A number of projects are figuring out how to grow their communities and transition to community-based governance. Some old and some new flavors are figuring out how to become sustainable and grow their contributor-base.  I\u0026rsquo;m very excited for what\u0026rsquo;s to come and I have a lot of trust in the power of the Ubuntu Community to keep growing and improving. If you yourself are lost or stuck, and want to discuss any community-related topics you want to discuss with me, feel free to send be an email or contact me on Matrix!\n",
    "ref": "/blog/2024-02-17-ubuntu-community-council-year/"
  },{
    "title": "Dell EcoLoop Pro vs Premier backpack",
    "date": "",
    "description": "",
    "body": "I bought the Dell EcoLoop Pro, so you don\u0026rsquo;t have to.\nDell EcoLoop Pro At the beginning of this year, I bought the Dell EcoLoop Pro Slim and regretted it immediately. The advertising for it is borderline deceptive, in my opinion.\nMy previous experience with Dell backpacks is that they look stylish and professional, are very handy, and very durable. I\u0026rsquo;ve used my previous Dell backpack for about 6 years, and it\u0026rsquo;s still very useful, although it\u0026rsquo;s starting to look a bit dated.\nWhen looking for a replacement, I landed on the Dell EcoLoop Pro (CP5723) backpack. The marketing materials show a stylish but versatile backpack that continues Dell\u0026rsquo;s great track record in this space.\n In the ads, the EcoLoop Pro looks super slick\n The reality, however, is far from it. What I received looked like a cheap knock-off version of what the listing showed. While the backpack is functionally the same, the exterior fabric looks a lot cheaper and crumpled up. The stiff, professional, modern look that sold me is nowhere to be found. At first I thought this must have been a mistake, but after checking again, it\u0026rsquo;s clear that this is the EcoLoop Pro backpack. The material just looks completely different in real life.\n In reality, the EcoLoop Pro is a crumpled-up mess\n After comparing third-party pictures and video\u0026rsquo;s of Dell\u0026rsquo;s backpacks with their marketing materials, I found that this was was an issue with the entire EcoLoop Pro line, but not with EcoLoop Premier!\nDell EcoLoop Premier So I bought the Dell EcoLoop Premier backpack, and I\u0026rsquo;m a fan! This is an incredibly high-quality backpack and it still looks amazing, almost a year after I bought it! The Premier backpack actually looks like the promo pictures in real life.\n In reality, the EcoLoop Premier looks super nice!\n Note that I\u0026rsquo;ve been heavily using this backpack for almost one year. I use it almost every single day when commuting to my job by bike, and I\u0026rsquo;ve taken it on a number of flights. I\u0026rsquo;m not the most careful person in the world, but it could handle me! The only complaint I have is that one of the leather straps on the zipper broke. But that\u0026rsquo;s easily replacable.\nIn conclusion While the Dell EcoLoop Pro is a useful backpack, it looks terrible and crumpled up. The EcoLoop Premier, however, looks great and is very sturdy. Both backpacks are useful due to their many compartments and carry-on strap, but the EcoLoop Premier is definitely worth its price if you care about the looks and sturdyness of your backpack.\n Side-by-side comparison for increased effect. EcoLoop Pro Slim on the left, EcoLoop Premier Slim on the right.\n ",
    "ref": "/blog/2023-12-28-ecoloop-pro-vs-ecoloop-premier-slim/"
  },{
    "title": "Talks & Teaching",
    "date": "",
    "description": "",
    "body": "This is a selection of the talks and guest lectures I gave in the past.\nI love sharing my knowledge, both with peers and with the wider public! Contact me if you’re looking for a guest lecturer on one of these topics.\nCommunity building   Do-ocratic communities: when merit is not the only metric (recording) Ubuntu Summit, November 2023\n  How to solve conflict in a community of equals (recording) rC3, December 2020\n  Building Communities with Do-ocracy - Hackerspace Design (recording) 36c3 Podcast, December 2019\n  Do-ocracy done well (recording) 36c3, December 2019\n    -- WebAssembly   Bringing orchestration to the edge with Wasm component model (recording) Wasm Research Day 2023, October 2023\n  Lightweight Kubernetes Operators with WebAssembly (recording) Fosdem 2023, Februari 2023\n  Snapcrafters  Bringing Windows Applications to Linux App Stores (recording) Ubuntu Summit, November 2023  AI   How open source software is changing the (AI) world (slides) Science Festival Ghent, November 2023\n  Interview Urgent.fm about AI track at Science Festival (recording) Science Festival Ghent, November 2023\n  ",
    "ref": "/talks/"
  },{
    "title": "Our students can use AI for their master's thesis - the policy",
    "date": "",
    "description": "",
    "body": "During my day job, I teach a number of computer science courses in the Information Engineering Technology programme at Ghent University. The release of ChatGPT and GPT-4 has a large impact on our education because it can get grades of above 80% on most exams and can effortlessly write essays on complex subjects. Students are aware of this, and are thus increasingly using Large Language Models (LLMs) in their education for a number of tasks.\n Writing code Explaining error messages Translate text Improving grammar and flow of texts  Rather than seeing this as \u0026ldquo;cheating\u0026rdquo;, our department has decided to embrace ChatGPT as much as possible. AI is becoming an increasingly important part of society and it is our responsibility to prepare students for that. This includes teaching them how to use AI effectively and responsibly. This is in line with recent guidelines from the European University Association.\n \u0026ldquo;The higher education sector must adapt its learning, teaching and assessment approaches in such a way that AI is used effectively and appropriately. Universities must explore the responsible use of AI tools, in line with their mission, goals and values, and paying due regard to their legal framework and the broader consequences for and impacts on society, culture and the economy.\u0026rdquo;\n Our first step in this embrace is to officially allow the use of AI for a master\u0026rsquo;s thesis. Below is the policy as communicated to the students. I publish it here with the hope of gathering some broader feedback on it and possibly help other educators navigate this difficult terrain. The policy specifically requires students to document how they used AI in their master\u0026rsquo;s thesis, so hopefully in a few months time, I\u0026rsquo;ll be able to report back here and give an overview of how students used these tools and what their results were.\nPolicy as communicated to students This guideline explain how students of Master of Science in Information Engineering Technology at Ghent University can use AI programs such as ChatGPT, GitHub CoPilot and Google Bard in their master’s thesis.\nThe role of AI in digital society is increasing rapidly. Therefore, we want to teach students how to use AI responsibly and ethically. For this reason, using AI to write a master’s thesis and code is allowed, but the student remains responsible for the content and needs to add an attachment to their thesis detailing extensively how AI was used. It should explain which programs where used, what they were used for, and give some examples of the used prompts. Failure to do so is in violation of the academic integrity rules of our faculty.\nBe aware of the limitations of AI and take the necessary precautions.\n If you provide minimum effort prompts, you might get low quality results. Refine your prompts in order to get good outcomes. Experiment with this. Don’t trust anything it says. If it gives you a fact, assume it is wrong unless you can check it with another source. If it gives you a reference, read the reference to see if it exists and if it agrees with the generated text. You are responsible for any errors or omissions. This type of AI works best for topics you understand. Be aware that some low-effort prompts return very similar results to different users. Other prompts might return text straight from other sources without proper citation. You are responsible to make sure your use of AI does not result in plagiarism. For this reason, it is not advised to let the AI write entire paragraphs from scratch. Do not give sensitive or personal information to online AI. If your thesis is in collaboration with a company, discuss with them what kind of information you are allowed to share with the AI.  For tips on how to effectively use AI; take a look at How to use ChatGPT to boost your writing. Note that there are a number of interesting AI projects that can help you with writing. Some projects such as Grammarly are a lot more objective and hallucinate less.\nSources and inspiration   All my classes suddenly became AI classes\n  My class required AI. Here\u0026rsquo;s what I\u0026rsquo;ve learned so far.\n  European University Association: Artificial intelligence tools and their responsible use in higher education learning and teaching\n  Header images generated by Craiyon\n  ",
    "ref": "/blog/2023-05-05-ai-chatgpt-master-thesis/"
  },{
    "title": "About Merlijn Sebrechts",
    "date": "",
    "description": "",
    "body": "PhD holder and certified cyborg. I enjoy weird food, hiking, and games with a good story. I want to make the world a better place.\nI\u0026rsquo;m a senior researcher at imec and teaching fellow at Ghent University in the IDLab research group in Belgium.\nResearch My research is focussed on service orchestration in the cloud, edge and fog.\n Making it easier to deploy and manage complex applications in the cloud and edge. Enabling multiple independent parties to collaboratively build and maintain distributed software. Simplifying the operations surrounding machine-to-machine communication.  I have experience working on numerous research projects, both national (imec and VLAIO) and international (FP7, Horizon 2020, Grant for the Web), with industry partners, the public sector, and other academic institutions.\nSee my full publication record on ResearchGate.\nTeaching I give lectures and hands-on labs at Ghent University and UGain on a number of topics such as\n Infrastructure as Code Systems Design Distributed Data Processing OS and network security Open Source community and licensing Programming in C, C++ and Python.  Community building I\u0026rsquo;ve been a board member of Hackerspace.Gent since 2014 and wrote the Hackerspace Blueprint; a booklet about how to run a leaderless organization using do-ocracy. I regularly give talks and workshops about about community building and conflict resolution. I was elected to the Ubuntu Community Council in 2022.\nOpen Source I\u0026rsquo;m a passionate user of and contributor to open source. Some of the stuff I\u0026rsquo;ve worked on:\n I\u0026rsquo;m maintaining a bunch of Snap packages including Signal Desktop, Arduino IDE, PhotoScape and Easy OpenVPN Server. I contribute to the snapcraft.io ecosystem by writing documentation, blog posts, and PR\u0026rsquo;s for snapcraft and snapd. I helped kickstart the Ubuntu community theme initiative, although my biggest contribution there might have been suggesting to call it \u0026ldquo;Yaru\u0026rdquo;. I used to be very active in the Juju community, writing a bunch of Big Data Charms as part of the Tengu Team, writing a bunch of documentation, and helping design the 2.0 version of the charms.reactive framework and writing a paper about how cool it is. Had the great pleasure of being appreciated by Jorge Castro and can confirm he is a Chinese botnet. I answered a bunch of questions on AskUbuntu.  Family Proud fiancé of Anne Fonteyn and loving dad of two cats: Simba the floofball and Panda the gooby baby.\n",
    "ref": "/about/"
  },{
    "title": "Using Dell MH3021P speakerphone on Ubuntu Linux",
    "date": "",
    "description": "",
    "body": "I recently bought the Dell MH3021P speakerphone. I want to use it for hybrid meetings and for recording audio during lectures. Dell doesn\u0026rsquo;t say the device supports Linux, so I was curious to see how much of the device works. Google didn\u0026rsquo;t have an answer so I bought the device to test it for myself.\nThankfully I can say all the important parts work very well out of the box on Ubuntu 22.04!\nWhat works on Linux  Using the speaker and microphone from the device for calls. The audio is very clear and picks up everyone\u0026rsquo;s voice when it\u0026rsquo;s sitting in the middle of the table. Using the internal USB-C docking station. HDMI output, regular USB ports and USB-c ports all work as expected! Charging your laptop from the speakerphone. Connect your charger to the speakerphone, connect the speakerphone to your laptop and your laptop starts charging! The volume buttons on the speakerphone change the volume of my Ubuntu laptop (with GNOME).  What does not work on Linux  The mute button does nothing. I tried it with both Zoom and Microsoft Teams, and it doesn\u0026rsquo;t seem to work. Moreover, it doesn\u0026rsquo;t even mute its own microphone. The \u0026ldquo;accept call\u0026rdquo; and \u0026ldquo;hangup\u0026rdquo; button don\u0026rsquo;t do anything either. I also tested this with Zoom and Microsoft Teams.  If you are interested in figuring out why these things don\u0026rsquo;t work and how to get them to work, let me know in the comments or send me an email!\nAppendix: device logs and info In case this is of interest or useful to anyone, these are some logs that give a bit more information about the internals of the device.\n$ lsusb Bus 003 Device 014: ID 2109:8884 VIA Labs, Inc. Dell MH3021P Bus 003 Device 013: ID 413c:81e3 Dell Computer Corp. Dell MH3021P Bus 003 Device 012: ID 2109:2822 VIA Labs, Inc. USB2.0 Hub Bus 002 Device 007: ID 2109:0822 VIA Labs, Inc. USB3.1 Hub $ sudo journalctl -k Jan 24 11:38:15 laptop kernel: usb 3-2: new high-speed USB device number 23 using xhci_hcd Jan 24 11:38:15 laptop kernel: usb 2-2: new SuperSpeed Plus Gen 2x1 USB device number 10 using xhci_hcd Jan 24 11:38:15 laptop kernel: usb 2-2: New USB device found, idVendor=2109, idProduct=0822, bcdDevice= 6.33 Jan 24 11:38:15 laptop kernel: usb 2-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3 Jan 24 11:38:15 laptop kernel: usb 2-2: Product: USB3.1 Hub Jan 24 11:38:15 laptop kernel: usb 2-2: Manufacturer: VIA Labs, Inc. Jan 24 11:38:15 laptop kernel: usb 2-2: SerialNumber: 000000001 Jan 24 11:38:15 laptop kernel: hub 2-2:1.0: USB hub found Jan 24 11:38:15 laptop kernel: hub 2-2:1.0: 4 ports detected Jan 24 11:38:15 laptop kernel: usb 3-2: New USB device found, idVendor=2109, idProduct=2822, bcdDevice= 6.33 Jan 24 11:38:15 laptop kernel: usb 3-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3 Jan 24 11:38:15 laptop kernel: usb 3-2: Product: USB2.0 Hub Jan 24 11:38:15 laptop kernel: usb 3-2: Manufacturer: VIA Labs, Inc. Jan 24 11:38:15 laptop kernel: usb 3-2: SerialNumber: 000000001 Jan 24 11:38:15 laptop kernel: hub 3-2:1.0: USB hub found Jan 24 11:38:15 laptop kernel: hub 3-2:1.0: 5 ports detected Jan 24 11:38:17 laptop kernel: usb 3-2.3: new full-speed USB device number 24 using xhci_hcd Jan 24 11:38:17 laptop kernel: usb 3-2.3: New USB device found, idVendor=413c, idProduct=81e3, bcdDevice= 0.69 Jan 24 11:38:17 laptop kernel: usb 3-2.3: New USB device strings: Mfr=3, Product=1, SerialNumber=2 Jan 24 11:38:17 laptop kernel: usb 3-2.3: Product: Dell MH3021P Jan 24 11:38:17 laptop kernel: usb 3-2.3: Manufacturer: Luxshar Jan 24 11:38:17 laptop kernel: usb 3-2.3: SerialNumber: 20210121.69.0.0 Jan 24 11:38:17 laptop kernel: input: Luxshar Dell MH3021P as /devices/pci0000:00/0000:00:14.0/usb3/3-2/3-2.3/3-2.3:1.3/0003:413C:81E3.000F/input/input50 Jan 24 11:38:17 laptop kernel: input: Luxshar Dell MH3021P Consumer Control as /devices/pci0000:00/0000:00:14.0/usb3/3-2/3-2.3/3-2.3:1.3/0003:413C:81E3.000F/input/input51 Jan 24 11:38:17 laptop kernel: input: Luxshar Dell MH3021P as /devices/pci0000:00/0000:00:14.0/usb3/3-2/3-2.3/3-2.3:1.3/0003:413C:81E3.000F/input/input53 Jan 24 11:38:17 laptop kernel: hid-generic 0003:413C:81E3.000F: input,hiddev3,hidraw7: USB HID v1.11 Device [Luxshar Dell MH3021P] on usb-0000:00:14.0-2.3/input3 Jan 24 11:38:20 laptop kernel: hub 2-2:1.0: set hub depth failed Jan 24 11:38:20 laptop kernel: usb 3-2.5: new high-speed USB device number 25 using xhci_hcd Jan 24 11:38:20 laptop kernel: usb 3-2.5: New USB device found, idVendor=2109, idProduct=8884, bcdDevice= 0.01 Jan 24 11:38:20 laptop kernel: usb 3-2.5: New USB device strings: Mfr=1, Product=2, SerialNumber=3 Jan 24 11:38:20 laptop kernel: usb 3-2.5: Product: Dell MH3021P Jan 24 11:38:20 laptop kernel: usb 3-2.5: Manufacturer: VIA Labs, Inc. Jan 24 11:38:20 laptop kernel: usb 3-2.5: SerialNumber: 0000000000000001 Jan 24 11:38:20 laptop kernel: usb 2-2: USB disconnect, device number 10 Jan 24 11:38:20 laptop kernel: usb 2-2: new SuperSpeed Plus Gen 2x1 USB device number 11 using xhci_hcd Jan 24 11:38:20 laptop kernel: usb 2-2: New USB device found, idVendor=2109, idProduct=0822, bcdDevice= 6.33 Jan 24 11:38:20 laptop kernel: usb 2-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3 Jan 24 11:38:20 laptop kernel: usb 2-2: Product: USB3.1 Hub Jan 24 11:38:20 laptop kernel: usb 2-2: Manufacturer: VIA Labs, Inc. Jan 24 11:38:20 laptop kernel: usb 2-2: SerialNumber: 000000001 Jan 24 11:38:20 laptop kernel: hub 2-2:1.0: USB hub found Jan 24 11:38:20 laptop kernel: hub 2-2:1.0: 4 ports detected $ usb-devices T: Bus=02 Lev=01 Prnt=01 Port=01 Cnt=01 Dev#= 11 Spd=10000 MxCh= 4 D: Ver= 3.20 Cls=09(hub ) Sub=00 Prot=03 MxPS= 9 #Cfgs= 1 P: Vendor=2109 ProdID=0822 Rev=06.33 S: Manufacturer=VIA Labs, Inc. S: Product=USB3.1 Hub S: SerialNumber=000000001 C: #Ifs= 1 Cfg#= 1 Atr=e0 MxPwr=0mA I: If#= 0 Alt= 0 #EPs= 1 Cls=09(hub ) Sub=00 Prot=00 Driver=hub E: Ad=81(I) Atr=13(Int.) MxPS= 2 Ivl=16ms $ sudo lsusb -v Bus 003 Device 028: ID 2109:8884 VIA Labs, Inc. Dell MH3021P Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 2.01 bDeviceClass 254 Application Specific Interface bDeviceSubClass 0 bDeviceProtocol 0 bMaxPacketSize0 64 idVendor 0x2109 VIA Labs, Inc. idProduct 0x8884 bcdDevice 0.01 iManufacturer 1 VIA Labs, Inc. iProduct 2 Dell MH3021P iSerial 3 0000000000000001 bNumConfigurations 1 Configuration Descriptor: bLength 9 bDescriptorType 2 wTotalLength 0x0012 bNumInterfaces 1 bConfigurationValue 1 iConfiguration 3 0000000000000001 bmAttributes 0xc0 Self Powered MaxPower 100mA Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 0 bNumEndpoints 0 bInterfaceClass 17 bInterfaceSubClass 0 bInterfaceProtocol 0 iInterface 3 0000000000000001 Binary Object Store Descriptor: bLength 5 bDescriptorType 15 wTotalLength 0x0051 bNumDeviceCaps 3 Billboard Capability: bLength 48 bDescriptorType 16 bDevCapabilityType 13 iAdditionalInfoURL 3 0000000000000001 bNumberOfAlternateModes 1 bPreferredAlternateMode 0 VCONN Power 0 1W bmConfigured 03 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 bcdVersion 1.21 bAdditionalFailureInfo 0 bReserved 0 Alternate Modes supported by Device Container: Alternate Mode 0 : Alternate Mode configuration successful wSVID[0] 0xFF01 bAlternateMode[0] 0 iAlternateModeString[0] 3 0000000000000001 Billboard Alternate Mode Capability: bLength 8 bDescriptorType 16 bDevCapabilityType 15 bIndex 0 dwAlternateModeVdo 0x05000000 Container ID Device Capability: bLength 20 bDescriptorType 16 bDevCapabilityType 4 bReserved 0 ContainerID {30eef35c-07d5-2549-b001-802d79434c30} Device Status: 0x0001 Self Powered Bus 003 Device 027: ID 413c:81e3 Dell Computer Corp. Dell MH3021P Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 1.10 bDeviceClass 0 bDeviceSubClass 0 bDeviceProtocol 0 bMaxPacketSize0 64 idVendor 0x413c Dell Computer Corp. idProduct 0x81e3 bcdDevice 0.69 iManufacturer 3 Luxshar iProduct 1 Dell MH3021P iSerial 2 20210121.69.0.0 bNumConfigurations 1 Configuration Descriptor: bLength 9 bDescriptorType 2 wTotalLength 0x00f9 bNumInterfaces 4 bConfigurationValue 1 iConfiguration 0 bmAttributes 0xa0 (Bus Powered) Remote Wakeup MaxPower 100mA Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 0 bNumEndpoints 1 bInterfaceClass 1 Audio bInterfaceSubClass 1 Control Device bInterfaceProtocol 0 iInterface 0 AudioControl Interface Descriptor: bLength 10 bDescriptorType 36 bDescriptorSubtype 1 (HEADER) bcdADC 1.00 wTotalLength 0x005d bInCollection 2 baInterfaceNr(0) 1 baInterfaceNr(1) 2 AudioControl Interface Descriptor: bLength 12 bDescriptorType 36 bDescriptorSubtype 2 (INPUT_TERMINAL) bTerminalID 1 wTerminalType 0x0405 Echo-canceling speakerphone bAssocTerminal 0 bNrChannels 2 wChannelConfig 0x0003 Left Front (L) Right Front (R) iChannelNames 0 iTerminal 0 AudioControl Interface Descriptor: bLength 13 bDescriptorType 36 bDescriptorSubtype 6 (FEATURE_UNIT) bUnitID 3 bSourceID 1 bControlSize 2 bmaControls(0) 0x0001 Mute Control bmaControls(1) 0x0002 Volume Control bmaControls(2) 0x0002 Volume Control iFeature 0 AudioControl Interface Descriptor: bLength 9 bDescriptorType 36 bDescriptorSubtype 3 (OUTPUT_TERMINAL) bTerminalID 2 wTerminalType 0x0101 USB Streaming bAssocTerminal 1 bSourceID 4 iTerminal 0 AudioControl Interface Descriptor: bLength 15 bDescriptorType 36 bDescriptorSubtype 8 (EXTENSION_UNIT) bUnitID 4 wExtensionCode 0x0bda bNrInPins 1 baSourceID(0) 3 bNrChannels 2 wChannelConfig 0x0003 Left Front (L) Right Front (R) iChannelNames 0 bControlSize 1 bmControls(0) 0x01 iExtension 0 AudioControl Interface Descriptor: bLength 12 bDescriptorType 36 bDescriptorSubtype 2 (INPUT_TERMINAL) bTerminalID 14 wTerminalType 0x0101 USB Streaming bAssocTerminal 0 bNrChannels 2 wChannelConfig 0x0003 Left Front (L) Right Front (R) iChannelNames 0 iTerminal 0 AudioControl Interface Descriptor: bLength 9 bDescriptorType 36 bDescriptorSubtype 3 (OUTPUT_TERMINAL) bTerminalID 15 wTerminalType 0x0405 Echo-canceling speakerphone bAssocTerminal 14 bSourceID 16 iTerminal 0 AudioControl Interface Descriptor: bLength 13 bDescriptorType 36 bDescriptorSubtype 6 (FEATURE_UNIT) bUnitID 16 bSourceID 14 bControlSize 2 bmaControls(0) 0x0001 Mute Control bmaControls(1) 0x0002 Volume Control bmaControls(2) 0x0002 Volume Control iFeature 0 Endpoint Descriptor: bLength 9 bDescriptorType 5 bEndpointAddress 0x85 EP 5 IN bmAttributes 3 Transfer Type Interrupt Synch Type None Usage Type Data wMaxPacketSize 0x0010 1x 16 bytes bInterval 8 bRefresh 0 bSynchAddress 0 Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 1 bAlternateSetting 0 bNumEndpoints 0 bInterfaceClass 1 Audio bInterfaceSubClass 2 Streaming bInterfaceProtocol 0 iInterface 0 Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 1 bAlternateSetting 1 bNumEndpoints 1 bInterfaceClass 1 Audio bInterfaceSubClass 2 Streaming bInterfaceProtocol 0 iInterface 0 AudioStreaming Interface Descriptor: bLength 7 bDescriptorType 36 bDescriptorSubtype 1 (AS_GENERAL) bTerminalLink 2 bDelay 1 frames wFormatTag 0x0001 PCM AudioStreaming Interface Descriptor: bLength 11 bDescriptorType 36 bDescriptorSubtype 2 (FORMAT_TYPE) bFormatType 1 (FORMAT_TYPE_I) bNrChannels 2 bSubframeSize 2 bBitResolution 16 bSamFreqType 1 Discrete tSamFreq[ 0] 48000 Endpoint Descriptor: bLength 9 bDescriptorType 5 bEndpointAddress 0x81 EP 1 IN bmAttributes 5 Transfer Type Isochronous Synch Type Asynchronous Usage Type Data wMaxPacketSize 0x00c0 1x 192 bytes bInterval 1 bRefresh 0 bSynchAddress 0 AudioStreaming Endpoint Descriptor: bLength 7 bDescriptorType 37 bDescriptorSubtype 1 (EP_GENERAL) bmAttributes 0x01 Sampling Frequency bLockDelayUnits 0 Undefined wLockDelay 0x0000 Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 2 bAlternateSetting 0 bNumEndpoints 0 bInterfaceClass 1 Audio bInterfaceSubClass 2 Streaming bInterfaceProtocol 0 iInterface 0 Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 2 bAlternateSetting 1 bNumEndpoints 1 bInterfaceClass 1 Audio bInterfaceSubClass 2 Streaming bInterfaceProtocol 0 iInterface 0 AudioStreaming Interface Descriptor: bLength 7 bDescriptorType 36 bDescriptorSubtype 1 (AS_GENERAL) bTerminalLink 14 bDelay 1 frames wFormatTag 0x0001 PCM AudioStreaming Interface Descriptor: bLength 11 bDescriptorType 36 bDescriptorSubtype 2 (FORMAT_TYPE) bFormatType 1 (FORMAT_TYPE_I) bNrChannels 2 bSubframeSize 2 bBitResolution 16 bSamFreqType 1 Discrete tSamFreq[ 0] 48000 Endpoint Descriptor: bLength 9 bDescriptorType 5 bEndpointAddress 0x07 EP 7 OUT bmAttributes 9 Transfer Type Isochronous Synch Type Adaptive Usage Type Data wMaxPacketSize 0x00c0 1x 192 bytes bInterval 1 bRefresh 0 bSynchAddress 0 AudioStreaming Endpoint Descriptor: bLength 7 bDescriptorType 37 bDescriptorSubtype 1 (EP_GENERAL) bmAttributes 0x01 Sampling Frequency bLockDelayUnits 0 Undefined wLockDelay 0x0000 Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 3 bAlternateSetting 0 bNumEndpoints 1 bInterfaceClass 3 Human Interface Device bInterfaceSubClass 0 bInterfaceProtocol 0 iInterface 0 HID Device Descriptor: bLength 9 bDescriptorType 33 bcdHID 1.11 bCountryCode 0 Not supported bNumDescriptors 1 bDescriptorType 34 Report wDescriptorLength 504 Report Descriptors: ** UNAVAILABLE ** Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x86 EP 6 IN bmAttributes 3 Transfer Type Interrupt Synch Type None Usage Type Data wMaxPacketSize 0x0010 1x 16 bytes bInterval 8 Device Status: 0x0002 (Bus Powered) Remote Wakeup Enabled Bus 003 Device 026: ID 2109:2822 VIA Labs, Inc. USB2.0 Hub Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 2.10 bDeviceClass 9 Hub bDeviceSubClass 0 bDeviceProtocol 2 TT per port bMaxPacketSize0 64 idVendor 0x2109 VIA Labs, Inc. idProduct 0x2822 bcdDevice 6.33 iManufacturer 1 VIA Labs, Inc. iProduct 2 USB2.0 Hub iSerial 3 000000001 bNumConfigurations 1 Configuration Descriptor: bLength 9 bDescriptorType 2 wTotalLength 0x0029 bNumInterfaces 1 bConfigurationValue 1 iConfiguration 0 bmAttributes 0xe0 Self Powered Remote Wakeup MaxPower 0mA Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 0 bNumEndpoints 1 bInterfaceClass 9 Hub bInterfaceSubClass 0 bInterfaceProtocol 1 Single TT iInterface 0 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x81 EP 1 IN bmAttributes 3 Transfer Type Interrupt Synch Type None Usage Type Data wMaxPacketSize 0x0001 1x 1 bytes bInterval 12 Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 1 bNumEndpoints 1 bInterfaceClass 9 Hub bInterfaceSubClass 0 bInterfaceProtocol 2 TT per port iInterface 0 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x81 EP 1 IN bmAttributes 3 Transfer Type Interrupt Synch Type None Usage Type Data wMaxPacketSize 0x0001 1x 1 bytes bInterval 12 Hub Descriptor: bLength 9 bDescriptorType 41 nNbrPorts 5 wHubCharacteristic 0x00e9 Per-port power switching Per-port overcurrent protection TT think time 32 FS bits Port indicators bPwrOn2PwrGood 175 * 2 milli seconds bHubContrCurrent 100 milli Ampere DeviceRemovable 0x20 PortPwrCtrlMask 0xff Hub Port Status: Port 1: 0000.0100 power Port 2: 0000.0100 power Port 3: 0000.0103 power enable connect Port 4: 0000.0100 power Port 5: 0000.0503 highspeed power enable connect Binary Object Store Descriptor: bLength 5 bDescriptorType 15 wTotalLength 0x0049 bNumDeviceCaps 5 USB 2.0 Extension Device Capability: bLength 7 bDescriptorType 16 bDevCapabilityType 2 bmAttributes 0x00000006 BESL Link Power Management (LPM) Supported SuperSpeed USB Device Capability: bLength 10 bDescriptorType 16 bDevCapabilityType 3 bmAttributes 0x00 wSpeedsSupported 0x000e Device can operate at Full Speed (12Mbps) Device can operate at High Speed (480Mbps) Device can operate at SuperSpeed (5Gbps) bFunctionalitySupport 1 Lowest fully-functional device speed is Full Speed (12Mbps) bU1DevExitLat 4 micro seconds bU2DevExitLat 231 micro seconds Container ID Device Capability: bLength 20 bDescriptorType 16 bDevCapabilityType 4 bReserved 0 ContainerID {30eef35c-07d5-2549-b001-802d79434c30} SuperSpeedPlus USB Device Capability: bLength 28 bDescriptorType 16 bDevCapabilityType 10 bmAttributes 0x00000023 Sublink Speed Attribute count 3 Sublink Speed ID count 1 wFunctionalitySupport 0x1100 bmSublinkSpeedAttr[0] 0x00050030 Speed Attribute ID: 0 5Gb/s Symmetric RX SuperSpeed bmSublinkSpeedAttr[1] 0x000500b0 Speed Attribute ID: 0 5Gb/s Symmetric TX SuperSpeed bmSublinkSpeedAttr[2] 0x000a4031 Speed Attribute ID: 1 10Gb/s Symmetric RX SuperSpeedPlus bmSublinkSpeedAttr[3] 0x000a40b1 Speed Attribute ID: 1 10Gb/s Symmetric TX SuperSpeedPlus ** UNRECOGNIZED: 03 10 0b Device Status: 0x0001 Self Powered Bus 002 Device 013: ID 2109:0822 VIA Labs, Inc. USB3.1 Hub Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 3.20 bDeviceClass 9 Hub bDeviceSubClass 0 bDeviceProtocol 3 bMaxPacketSize0 9 idVendor 0x2109 VIA Labs, Inc. idProduct 0x0822 bcdDevice 6.33 iManufacturer 1 VIA Labs, Inc. iProduct 2 USB3.1 Hub iSerial 3 000000001 bNumConfigurations 1 Configuration Descriptor: bLength 9 bDescriptorType 2 wTotalLength 0x001f bNumInterfaces 1 bConfigurationValue 1 iConfiguration 0 bmAttributes 0xe0 Self Powered Remote Wakeup MaxPower 0mA Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 0 bNumEndpoints 1 bInterfaceClass 9 Hub bInterfaceSubClass 0 bInterfaceProtocol 0 Full speed (or root) hub iInterface 0 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x81 EP 1 IN bmAttributes 19 Transfer Type Interrupt Synch Type None Usage Type Feedback wMaxPacketSize 0x0002 1x 2 bytes bInterval 8 bMaxBurst 0 Hub Descriptor: bLength 12 bDescriptorType 42 nNbrPorts 4 wHubCharacteristic 0x0009 Per-port power switching Per-port overcurrent protection bPwrOn2PwrGood 175 * 2 milli seconds bHubContrCurrent 0 milli Ampere bHubDecLat 0.4 micro seconds wHubDelay 2292 nano seconds DeviceRemovable 0x00 Hub Port Status: Port 1: 0000.02a0 lowspeed L1 Port 2: 0000.02a0 lowspeed L1 Port 3: 0000.02a0 lowspeed L1 Port 4: 0000.02a0 lowspeed L1 Binary Object Store Descriptor: bLength 5 bDescriptorType 15 wTotalLength 0x0049 bNumDeviceCaps 5 USB 2.0 Extension Device Capability: bLength 7 bDescriptorType 16 bDevCapabilityType 2 bmAttributes 0x00000006 BESL Link Power Management (LPM) Supported SuperSpeed USB Device Capability: bLength 10 bDescriptorType 16 bDevCapabilityType 3 bmAttributes 0x00 wSpeedsSupported 0x000e Device can operate at Full Speed (12Mbps) Device can operate at High Speed (480Mbps) Device can operate at SuperSpeed (5Gbps) bFunctionalitySupport 1 Lowest fully-functional device speed is Full Speed (12Mbps) bU1DevExitLat 4 micro seconds bU2DevExitLat 231 micro seconds Container ID Device Capability: bLength 20 bDescriptorType 16 bDevCapabilityType 4 bReserved 0 ContainerID {30eef35c-07d5-2549-b001-802d79434c30} SuperSpeedPlus USB Device Capability: bLength 28 bDescriptorType 16 bDevCapabilityType 10 bmAttributes 0x00000023 Sublink Speed Attribute count 3 Sublink Speed ID count 1 wFunctionalitySupport 0x1100 bmSublinkSpeedAttr[0] 0x00050030 Speed Attribute ID: 0 5Gb/s Symmetric RX SuperSpeed bmSublinkSpeedAttr[1] 0x000500b0 Speed Attribute ID: 0 5Gb/s Symmetric TX SuperSpeed bmSublinkSpeedAttr[2] 0x000a4031 Speed Attribute ID: 1 10Gb/s Symmetric RX SuperSpeedPlus bmSublinkSpeedAttr[3] 0x000a40b1 Speed Attribute ID: 1 10Gb/s Symmetric TX SuperSpeedPlus ** UNRECOGNIZED: 03 10 0b Device Status: 0x000d Self Powered U1 Enabled U2 Enabled ",
    "ref": "/blog/2023-01-24-dell-mh3021p-speakerphone-linux/"
  },{
    "title": "You can finally disable Snap updates",
    "date": "",
    "description": "",
    "body": "During the Ubuntu Summit, a long-awaited feature was quietly released for preview: You can now completely turn off automatic updates of snaps.\n \u0026ldquo;The snap refresh --hold command holds, or postpones, snap updates for individual snaps, or for all snaps on the system, either indefinitely or for a specified period of time.\u0026rdquo;\n(currently only available in edge channel of snapd)\n This might sound like an obvious feature to many people, but it\u0026rsquo;s a continental shift in philosophy for the Snap developers.\nSnaps allow users to easily install Linux applications. By default, snaps automatically update to the newest version. Snapd, the service that manages snaps, checks for updates four times in a day. Although there are many ways to control when and how often updates are installed, it was not possible to completely turn off automatic updates.\nWait, what? Yes, this was a conscious choice by the developers. Outdated and insecure Linux system are a massive problem because they are weaponized into botnets that attack services and spread malware. In an attempt to help solve this issue, the Snap developers decided to simply make it impossible to turn off automatic updates. When Snap was initially released in 2014, automatic updates were so new to the Linux ecosystem that the developers feared every list of \u0026ldquo;10 things to do after installing \u0026lt;distro\u0026gt;\u0026rdquo; would include \u0026ldquo;turn off automatic updates\u0026rdquo;, making the issue of insecure Linux devices even larger.\nWhy now? Nowadays, however, automatic updates have become commonplace. Even Ubuntu server has been automatically installing security updates since late 2016! Over the years, the snap update mechanism has been continuously refined to ensure updates don\u0026rsquo;t break things and to allow updates to happen at a convenient time. As a result, adding the functionality to disable updates will probably not result in a new wave of insecure devices. Moreover, as Snap matures and enters large enterprises with IT teams dedicated to manually updating software, adding this feature is a logical next step to give IT teams the control and predictability they have come to expect from Linux systems.\n",
    "ref": "/blog/2022-11-10-turn-off-snap-updates/"
  },{
    "title": "Lightweight Kubernetes Operators with WebAssembly",
    "date": "",
    "description": "",
    "body": "We created a prototype that runs Kubernetes operators in WebAssembly (wasm) and suspends them to disk when they are not used.\nWhy Developers want to use Kubernetes in the edge, but it uses too much memory for most devices. Running k8s operators in wasm greatly reduces their memory overhead and allows us to start and stop them dynamically. This way, they only run when there is actually something to do.\nKubernetes Operators Operators are an important part of the Kubernetes control plane. They add additional functionality to Kubernetes using Custom Resources. The ArgoCD operators, for example, provide additional resources to create CI/CD pipelines, and Rook allows you to manage storage clusters such as Ceph.\nThe issue with operators is that they keep running even when there is nothing to do. Simply adding the ArgoCD functionality to a Kubernetes cluster will eat up RAM, even when you\u0026rsquo;re not actively using it.\nWebAssembly + WASI WebAssembly (wasm) is a binary format to run applications in lightweight virtual machines. Many compilers support wasm as a target next to arm and x86_64. The WebAssembly System Interface (WASI) is a standardized API for wasm apps to talk to the outside world. They\u0026rsquo;re the \u0026ldquo;System Calls\u0026rdquo; of the WebAssembly world.\nCombining these two gives you a very lightweight but very secure way to isolate applications. Each wasm app runs in their own sandbox and the runtime decides what external resources it can access. Moreover, wasm apps start up lightning fast!\nKubernetes Operators in WebAssembly In 2020, Markus Thömmes and Francesco Guardiani introduced the idea of running Kubernetes controllers in WebAssembly:\n Today I’m going to introduce you an idea Markus Thömmes and I had to package, deploy and operate Kubernetes controllers in a modern and efficient way that may have a fundamental impact on the Kubernetes ecosystem. - Kubernetes Controllers - A new hope\n Their prototype didn\u0026rsquo;t support unloading idle controllers, however, and hasn\u0026rsquo;t seen any activity in over 2 years. So, to continue this dream, we ported their prototype to wasmtime, and added the functionality to suspend idle operators to disk.\nPerformance We did a bunch of extensive performance tests to see whether this actually reduces the memory footprint of controllers. The results are published in our paper at IEEE CloudNet 2022. Two findings from the paper:\n  100 Rust operators running in our wasm runtime use 68% less memory than if they run in containerd.\n  Moreover, they use an additional 50% less memory when swapped to disk during idling.\n  Next steps Right now, this is a very rough prototype which shows the potential of the approach. It\u0026rsquo;s definitely not ready for any serious usage, though, so our first step is to get it into a more usable state. There are also a couple of new features planned:\n Predictive unloading/swapping to disk. Right now, a controller gets swapped to disk immediately after finishing a control loop. This creates a lot of overhead, however, for controllers which have more changes to process. We want to be smarter about when to unload controllers. Go support. Right now, we only support controllers written in Rust. Go is the default language for controllers, however, so we want to take a look at adding support for those controllers in the framework. Better integration in lightweight Kubernetes distributions. This framework would be a great addition to tiny k8s distributions such as the FLEDGE project, created by a colleague of mine.  More information  The runtime and the modifications to kube-rs are available on Github. We published a preprint of the paper introducing this prototype at IEEE CloudNet 2022. The raw results and analysis code is available on GitHub. For even more information, read Tim Ramlot\u0026rsquo;s master\u0026rsquo;s thesis about this framework.  Acknowledgements A great amount of thanks goes to Tim Ramlot, who developed the software as part of his master\u0026rsquo;s thesis, and Markus Thömmes and Francesco Guardiani for the initial idea and original Proof of Concept.\n",
    "ref": "/blog/2022-09-05-wasm-k8s-controllers/"
  },{
    "title": "Open Google Drive and Google Calendar as a specific user",
    "date": "",
    "description": "",
    "body": "Quick tip: did you know you can open Google Drive, Google Calendar and more as a specific user?\nJust add ?authuser= and your URL encoded email address behind the address.\nFor example,\n https://drive.google.com/?authuser=merlijn.sebrechts%40gmail.com opens Google Drive as user merlijn.sebrechts@gmail.com. https://calendar.google.com/?authuser=merlijn.sebrechts%40ugent.be opens Google Calendar as user merlijn.sebrechts@ugent.be.  This works with almost every google website, including gmail.\n Note: you can use an online tool to URL encode your email address.\n ",
    "ref": "/blog/2022-09-10-google-drive-calendar-specific-user/"
  },{
    "title": "Ghent University address book in Thunderbird",
    "date": "",
    "description": "",
    "body": "UGent address book support in Mozilla Thunderbird Ghent University has an LDAP address book which contains all students and staff. Adding this address book to Thunderbird is non-trivial, though. Follow these steps to add it.\n  Install the extensions \u0026ldquo;TbSync\u0026rdquo; and \u0026ldquo;Provider for Exchange ActiveSync\u0026rdquo;. (Press alt to show the menu and choose Edit \u0026gt; Preferences \u0026gt; Add-ons and Themes)\n  Open TBSync settings, click Account actions \u0026gt; Add New Account \u0026gt; Exchange ActiveSync.\n  Choose the option Microsoft Office 365.\n  A screen pops up to login into your UGent account. Fill this in. This supports two-factor authentication using OAuth SSO.\n  Click on Enable and synchronize this account, enable the Contacts resource, and click Synchronize now.\n  That\u0026rsquo;s it! Now when you compose a new email, Thunderbird will auto-suggest email addresses from the UGent address book.\nManually searching the address book You can also search the address book directly.\n  Start by clicking on the \u0026ldquo;go to address book\u0026rdquo; button.\n  Because the UGent address book is so large, Thunderbird doesn\u0026rsquo;t show anything until you start searching. Select the address book and type a name in the search bar to see the contacts.\n  ",
    "ref": "/blog/2021-08-31-ugent-thunderbird/"
  },{
    "title": "How to verify the source of a Snap package",
    "date": "",
    "description": "",
    "body": "Many Snap packages contain two files which allow users to verify what sources were used to build the package.\n snap/snapcraft.yaml is the \u0026ldquo;source\u0026rdquo; of the package. This file was used by snapcraft to build the package. snap/manifest.yaml is a \u0026ldquo;recording\u0026rdquo; of the build of that package. It is similar to the first file, but it includes a lot more information to pinpoint what exact sources were being used. It records the exact package versions of dependencies, the git commit of all source repositories, checksums of any downloaded binary and more.  The manifest is used by the Snap Store to check if Snap packages contain libraries with security vulnerabilities. If this is the case, the publisher gets an email asking them to rebuild the package.\nBoth these files are added by the snapcraft build tool when the environment variable SNAPCRAFT_BUILD_INFO is set during the build. This is automatically enabled for all snaps built by snapcraft.io, Launchpad and on GitHub.\nWhat do the manifests contain? Let\u0026rsquo;s look at Canonical\u0026rsquo;s Chromium snap as an example.\nIn snap/snapcraft.yaml, source explains what repository is used to build the package and stage-packages describe what dependencies are included in the snap.\ndesktop-gtk3: source: https://github.com/ubuntu/snapcraft-desktop-helpers.git source-subdir: gtk stage-packages: - libxkbcommon0 - ttf-ubuntu-font-family - shared-mime-info ... In snap/manifest.yaml, source-commit is added to narrow down what exact source was used for this package and stage-packages include the exact version of each dependency.\ndesktop-gtk3: source: https://github.com/ubuntu/snapcraft-desktop-helpers.git source-commit: 622e2aa7a840b3a7dbb6ea4d432d687d5cc2e8ef source-subdir: gtk stage-packages: - libxkbcommon0=0.8.2-1~ubuntu18.04.1 - ttf-ubuntu-font-family=1:0.83-2 - shared-mime-info=1.9-2 ... Finally, for snaps built on Launchpad and GitHub, snap/manifest.yaml also contains a URL to the build log.\nimage-info: build-request-id: lp-58361925 build-request-timestamp: \u0026#39;2020-08-05T11:55:18Z\u0026#39; build_url: https://launchpad.net/~osomon/+snap/chromium-snap-firstrun-notice/+build/1071421 How do I access these files? Currently, you need to download the snaps before you can access the manifest.\n If you don\u0026rsquo;t have the Snap installed, run snap download snap-name, which will download the snap to the current directory. You can then open the snap with any archive manager to look at the files. If you have the snap installed, you can find them in /snap/snap-name/current/snap/.  Can the manifests be doctored? Yes; a malicious publisher can change the contents of these files. If the snap was built on Launchpad or GitHub, however, you can verify if the manifest was changed since the package was built. This is the process to verify a snap build from Launchpad:\n Download the snap and look for the build_url in snap/manifest.yaml. Surf to the URL and download the snap in \u0026ldquo;Built files\u0026rdquo;. Calculate the checksum of both snaps and see if they match.  If the checksums match, you know the manifest was not changed since the package was built. On Launchpad, you can also see which machine the package was built on, the entire build log and the source of that build.\n Note: This process requires you to trust the information in Launchpad is correct. This is the same infrastructure that builds the regular Ubuntu packages and hosts their source code, however, so Ubuntu and all of its derivatives already depend on this information being correct.\n Can I trust any package built on Launchpad? No! It is still very important to trust the publisher of a package themselves. Because the publisher controls the entire source code of the application, it is impossible to stop them from including malicious code. Even if that code is built on Launchpad.\n Note: for this reason, I\u0026rsquo;m not convinced that it\u0026rsquo;s useful to force publishers to include manifests and/or use Launchpad. It would only give users a false sense of security.\n Snap is specifically designed to let developers publish their apps directly to users without a middleman. This also means that users should only install software from developers they trust. The Snap sandbox helps to reduce the damage a malicious or broken snap can do, but it does not completely protect you from harm.\nWhat if the Snap was built on GitHub? As James Henstridge pointed out in the comments, if the Snap was built on GitHub using the official actions, you can also verify the manifest. The process is a little bit more complicated, though.\n  Download the snap and look for the build_url in snap/manifest.yaml.\n  Surf to the URL go to the log of the \u0026ldquo;build\u0026rdquo; step and look at the Run snapcore/action-publish@v1 part for the \u0026ldquo;revision\u0026rdquo; number.\nRevision 13 of \u0026#39;asciidoc-link-check\u0026#39; created. Track Arch Channel Version Revision latest amd64 stable 1.0.14 13 candidate ↑ ↑ beta ↑ ↑ edge 1.0.13 10   Check the following things:\n The name of the snap you just downloaded is the same as in this log. The revision of the snap you just downloaded is the same as in this log. The workflow uses snapcore/action-build and snapcore/action-publish to build and publish the Snap.  If these three are true, then the manifest was not changed since the build because publishers cannot modify the Snap of a revision after it was uploaded. Every change creates a new revision number.\n   Note: This process requires you to trust that the information in the Snap Store and on GitHub is correct.\n Areas of improvement If you\u0026rsquo;re interested in transparency, trust and reproducibility of snaps, there are many areas you can work on to further improve this. From the top of my head, these are some of the things that can improve:\n To see the manifest, users need to download the snap and manually look through the files. It would be very useful if the Snap Store web frontend and the snap CLI command could show you the manifest and the data in it. Although the manifest records everything used to build the package, you can\u0026rsquo;t yet use this recording to reproduce the package. The snapcraft build tool needs to be modified so it can rebuild a snap from the manifest.yaml. You won\u0026rsquo;t be starting from scratch though, as snapcraft already supports many of the fields in the manifest. The manifest currently does not contain the source url of the repository which contains the snapcraft.yaml file itself. You can use Launchpad to find this repository, but it would be better to include this repository in the manifest itself. As far as I know, Launchpad and GitHub are currently the only build services for Snaps which include a link to the build log. It would be useful for others such as the KDE Neon build service to do the same thing.   Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-08-17-verify-snap/"
  },{
    "title": "How I became a cyborg pt2",
    "date": "",
    "description": "",
    "body": " This is part 2 of a series on how I became a cyborg. Warning: this article talks in details about heart surgery and shows post-op images.\nRead part 1 first\n We can rebuild him 2019-08-06: Waking up from heart surgery.\nThe first thing I saw were my parents talking to a nurse. My parents wanted to leave but the nurse asked them to stay, saying \u0026ldquo;it\u0026rsquo;s better to have familiar faces when he wakes up\u0026rdquo;. I nodded, but they didn\u0026rsquo;t really seem to notice so I just closed my eyes for a bit and waited until I was a bit more awake.\nWhat felt like minutes of drifting in and out of consciousness were actually hours according to my parent\u0026rsquo;s accounts. The more aware I got, the more I felt extremely hot. During surgery, they cooled my body down to 28 °C to slow my metabolic rate, so after surgery they mummified me in blankets to get my temperature back up but I really don\u0026rsquo;t like being hot.. Given that I could barely move any limb, wiggling my toes loose was a losing battle. Asking my parents for help didn\u0026rsquo;t work either because for some reason I wasn\u0026rsquo;t able to produce any sound. There was nothing left but to use my fingers to try to pull up the blanket but the nurse thwarted this attempt, holding my arm and telling me to lay still because I was still connected to dozens of tubes and sensors. The plus side was that I finally had the attention of the spectators, so I tried to use hand signals to ask them to help my feet escape from their personal sauna. After some frantic pointing they finally understood me and released my feet. Success! Hardest part of the recovery done!\nBy this point, my parents also saw that I tried to speak a few times so they explained to me that I was still intubated. No idea how I could have missed the giant tube going in my lungs. Annoyingly, they didn\u0026rsquo;t answer the single most important question of that moment: \u0026ldquo;How did the surgery go?!\u0026rdquo; Before they put me to sleep, I was under the impressions that I wouldn\u0026rsquo;t be able to see my parents until the day after surgery, so was seeing my parents a good or a bad sign? Were they able to fix the broken valve or did they have to put in a new one? Luckely, the nurse extubated me soon after, allowing me to finally pop the question: \u0026ldquo;So, how did it go?\u0026rdquo;\n\u0026ldquo;Oh?! -as if they were surprised by me aquiring what my current health status was- Everything went well. She wasn\u0026rsquo;t able to fix the valve, but they put in an artificial one and it\u0026rsquo;s working as it should.\u0026rdquo;\nFor some reason, I was kind of hoping for this outcome. Imagine how dumb it must\u0026rsquo;ve been to go through this entire ordeal and not even becoming a cyborg.. At least I had a cool new title now!\nTalking was still difficult for me because I simply couldn\u0026rsquo;t get enough air in my lungs to say a full sentence. I told my parents I was seeing the biggest aura I had ever seen. The nurse gave me a weird look so I explained \u0026ldquo;Like when I\u0026rsquo;m getting a migraine\u0026rdquo;.\n Your browser does not support the video tag.  This is what an aura normally looks like for me. I see this weird flashing shape before I get a migraine attack. It starts with a tiny spot and grows into a big spiral. The headache itself starts a few minutes after the aura disappears.\n The entire top part of my vision was flickering like this for the first few hours. Not that it was a big issue, but I found it very interesting that I had the biggest aura in my life after open heart surgery. It gave me some hope that my migraines were connected to my shitty heart and that they might become better with the new valve..\n 🕑 Now, a year later, I\u0026rsquo;m a bit disappointed by the effect it had on my headaches. Although they\u0026rsquo;re slightly better, I\u0026rsquo;m still plagued by them.\n After my parents left, the first night started and it was actually a relatively pleasant experience thanks to an amazing nurse who was with me the entire night. I wasn\u0026rsquo;t able to sleep much, though. Every time I fell asleep I seemed to stop breathing and woke back up gasping for air.. During the surgery they gave me medication to stop my breathing reflex. During open heart surgery, they stop your heart and lungs and connect your arteries to a machine which simulates them. For this, you need to stop breathing completely, and apparently it can take a while for everything to normalize again. The nurse was able to comfort me, however, by explaining that I had multiple sensors checking the oxygen level in my blood. She constantly adjusted the extra oxygen I got based on those values and turned the display to me so I could watch along. We talked a lot throughout the first night, which mostly consisted of me asking what every single tube and machine was doing and her explaining that.\n When my parents left, they took a picture to show my family and friends I was alive.\n The pain the first night was relatively ok. The nurse used a whole bunch of pillows to put my upper body in a position that hurt the least, which helped a lot, and the anesthesia was still very much in my body. As the night progressed, the pain got worse, but the nurse kept increasing the painkillers to keep it under control. Even though the pain was under control, I felt as if lying in that bed was one of the most difficult things I\u0026rsquo;ve ever done in my life. The simple act of breathing required a lot of effort. Given my ribs were just cut open, I had to focus on using my stomach to breathe instead of my chest and I had to fight the pain every time I inhaled. Not breathing would have been a much less painful option but most doctors recommend against that. Having to spend so much energy to do something so vital was a very weird experience. As a seemingly healthy 27-year old, I\u0026rsquo;ve never been confronted with the fact that my body can fail to do even the simplest things required to keep me alive. I still felt as if I could take it, though. I still had some fight left in me. I was even making some jokes! I heard my new valve tick and I explained how my childhood dream of becoming a ninja is now shattered because people will be able to hear me tick when I sneak up to them.\n 🕑 Today, a year in to living with this valve, I\u0026rsquo;m very happy my job doesn\u0026rsquo;t require sneaking around because other people really hear my heart tick when they stand in front of me in a quiet room.\n My mood quickly changed in the morning, however. There is a limit to how much painkillers they can pump through your body. Moreover, because we believed I was allergic to one of the painkillers, the options were even more limited for me. So we reached a point where they couldn\u0026rsquo;t increase the dosage to match my pain level anymore. On top of that, the morning brought in the morning crew, who were surprisingly successful in maintaining a noise level of \u0026ldquo;London underground during rush hour\u0026rdquo;. The morning crew also did an rx scan of my chest and found that part of my right lung collapsed. This is not what you want to hear when you feel like you\u0026rsquo;re only just able to breathe enough to survive.\n Somehow, this picture should show my right lung is collapsed. I can\u0026rsquo;t see it though.. The clamps are from sensors taped to me and the very thin wires going to my heart are for a temporary pace-maker. In the middle you see six tied up metal wires used to hold my ribs closed. You can see my new aorta valve to the right of the fourth wire from the top.\n When I told the nurse I had real difficulties with the noise, she explained they wanted to move me from the ICU to mid-care, which is a lot more quiet, but my blood pressure was still too high to do that. Even though my blood pressure kept lowering throughout the night, for some strange reason it shot back up when the stampede of interns started running through the ICU yelling at each other. The only way to get my blood pressure down was to give me a pill.\nThe pill came with only one side effect: me puking my guts out.\n Hey Merlijn, isn\u0026rsquo;t it very painful to puke your guts out when any chest movement causes shooting pain?\n Good question, attentive reader! This is indeed extremely painful. Furthermore, the relieved feeling you normally get after puking was absent. My body couldn\u0026rsquo;t even pretend like what just happened was useful. Any bit of fight left in my body quickly disappeared. Or in better terms: any bit of fight left in my consciousness quickly disappeared because to my own surprise, my body and mind just kept going even without my support. The orchestra continued playing even though the conductor gave up. I discovered my brain has an auto-pilot ready to take over when my consciousness gave up.\nAfter giving me the same pill again, I was able to hold it in with much effort and they were finally able to wheel me into what felt like the quietest place I have ever been to: the hallway next to the ICU. The road to mid-care was so serene but to my surprise, the mid-care was even better! The lights were even off so I could close my eyes and pretend I could sleep!\nTBC\n",
    "ref": "/blog/2020-steampunk-cyborg-2/"
  },{
    "title": "Why is there only one Snap Store?",
    "date": "",
    "description": "",
    "body": "Snap and Flatpak are the basis of two universal app stores for Linux: the Snap Store and Flathub. Interestingly, Flatpak has multiple repositories: Flathub is the main one but both Fedora and Elementary OS also host their own store. In contrast; there is only one Snap store. Why is that?\n Note: for an introduction into Flatpak and Snap, please read The future of Linux desktop application delivery is Flatpak and Snap.\n Snap is designed so each device only connects to a single store for three reasons:\n users can easily discover new applications, developers can easily publish their apps, and developing Snap itself is easier.  Canonical, the company behind Snap and Ubuntu, already tried the distributed approach and discovered its downsides. Before Snap, \u0026ldquo;Personal Package Archives\u0026rdquo; (PPAs) were the recommended way to get third-party software to Ubuntu users. The idea of these is that each developer creates a tiny repository which includes their app. Users get new software by adding the PPA and installing the software using their package manager. This is very similar to how Flatpak supports multiple \u0026ldquo;stores\u0026rdquo;.\nOn Android and iOS, users often find new software by opening the app store and searching for what they need. This is not possible with PPAs because the software in a PPA only shows up in the Ubuntu app store after a user adds it. Snap solves this by having a single repository with all available apps. Users discover new software straight from the app store instead of having to search the internet.\nMany developers also find creating and maintaining a repository too cumbersome. Canonical tried to make this as easy as possible for PPAs with Launchpad taking care of building and hosting the apps. Nevertheless, the process is inherently more complicated than uploading your app to a store. Although PPAs are still used by many developers, even more software is only available to download from a website.\nThese issues are not unknown to the Flatpak project, which is why Flathub was created: having a single store which contains all apps benefits everyone.\nGiven the downsides of the distributed approach, Canonical decided to invest engineering resources in other parts of Snap instead. Creating a distributed system is a multiplier to complexity; each feature becomes harder to implement. That said, the Snap developers made it clear in the past that if anyone is interested in implementing this feature in Snap, they can definitely do so.\nWill there ever be an alternative Snap store? Let\u0026rsquo;s hope so! Although Snap is designed so that each device only connects to a single store, multiple stores can still exist! A distribution like Manjaro could very well point Snap at their own store. Manjaro chose to use the Snap store hosted by Canonical because they like the advantages that gives them. In the future, however, Manjaro could switch to their own Snap store if that makes sense to them.\nThis is exactly what the Ubuntu Touch project did. Ubuntu Touch is a smartphone OS which uses \u0026ldquo;click\u0026rdquo;, the precursor to Snap. They initially used the click store hosted by Canonical but they created their their own store and switched to it completely when Canonical gave up on their attempt to break the smartphone duopoly.\nIs the Snap Store open source? Sadly, part of the Snap store is still closed source. Snap itself is completely open source and many parts of the Snap store are open source like the web-store front-end, the automatic review tools, the build service, the desktop store app, and many more. The back-end hosting the snaps, however, is still proprietary.\nOpen sourcing the Snap store back-end would require significant changes to it, according to Martin Wimpress of Canonical:\n [because of its history,] the Snap store now integrates with other areas of the Canonical infrastructure. So the Snap store isn\u0026rsquo;t a single thing. It\u0026rsquo;s not like this one piece of software that you can easily decouple from the rest of the machinery that powers the infrastructure at Canonical. So we can\u0026rsquo;t just pull it apart and separate it and say, \u0026ldquo;Here you go, here\u0026rsquo;s the open source Snap store.\n Canonical is doubtful that this investment would be worth it because of what happened with Launchpad. Although they invested significant resources in open sourcing Launchpad, there is still only one instance of Launchpad running and they have not received any significant contributions from non-Canonical employees.\nInterestingly, Canonical actually released an open-source prototype Snap store backend a few years ago, but there was very little interest from the community in in actually maintaining and running a second Snap store, so the project bit-rotted and became incompatible with the current Snap protocol.\nCan other distros curate packages? Linux Mint recently made headlines by blocking users from installing Snap on Linux Mint. One of the reasons they gave for this decision is that the centralization of Snap means Linux Mint cannot provide different versions of certain snaps for their users. They explain that this is possible with APT:\n Thanks to the way APT works, if a bug isn’t fixed upstream, Debian can fix it with a patch. If Debian doesn’t, Ubuntu can. If Ubuntu doesn’t Linux Mint can. If Linux Mint doesn’t, anyone can, and not only can they fix it, they can distribute it with a PPA.\n It\u0026rsquo;s understandable that a distribution wants some level of control over what packages their users get but this is already possible with the Snap store!\n\u0026ldquo;Brand stores\u0026rdquo; are special copies of the Snap store where admins select which packages from the main store are available and which additional or modified packages are included. The European Space Agency, for example, has a brand store specific to satcom research. The downside of Brand stores is that you\u0026rsquo;re still using the infrastructure of Canonical, but Linux Mint is no stranger to this. Like most Ubuntu derivatives, they heavily relied on Canonical infrastructure when they first started out, even using the regular Ubuntu repositories and mirrors.\nAnother option would be to create their own Snap store with similar functionality and Canonical\u0026rsquo;s previous behavior suggests they would be open to helping that effort.\nMy thoughts about this Having a single Snap store which has closed source parts is very controversial in some parts of the Linux community. The decision to have only a single store per device seems reasonable to me: it has clear advantages both to users and developers. Still, I am happy Flatpak exists as a competing project with a decentralized design and Elementary OS\u0026rsquo;s hybrid approach does a great job of taking advantage of it. I hope that someday the community creates an alternative Snap store similar to what F-Droid is doing.\nI do not like the fact that the Snap store backend is proprietary because it is becoming such a core part of Ubuntu. I don\u0026rsquo;t think there is anything wrong with Canonical producing closed-source add-ons on top of Ubuntu in an \u0026ldquo;Open Core\u0026rdquo; fashion. GitLab does this really well and we have an amazing open source GitHub alternative because of it. There is nothing wrong with Canonical trying to make money; Ubuntu would not be here without them. Linux is great because of all the companies surrounding it.\nWhat I do take issue with is that you currently cannot use Snap for its intended purpose without this proprietary back-end. Ironically, Ubuntu\u0026rsquo;s former community member, Jono Bacon, made it very clear in a recent video that the proprietary parts of an Open Core project need to be optional. Having paid enterprise features such as the Snap Store Proxy is fine, for example, because Snap is still completely functional without it.\nThat said, I\u0026rsquo;m pragmatic about the proprietary back-end. DockerHub and GitHub are insanely popular and they are completely proprietary. Moreover, Canonical is much more receptive to community pressure than Docker inc or GitHub so we are in a much better position to steer them towards doing \u0026ldquo;the right thing\u0026rdquo;. Snap offers a lot of advantages compared to APT and compared to Flatpak, and if things really don\u0026rsquo;t work out with Canonical, we can always build our own back-end.\n Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-08-02-why-one-snap-store/"
  },{
    "title": "Copyleft and LGPL in Snaps and Flatpaks",
    "date": "",
    "description": "",
    "body": "The question often comes up of how Snaps and Flatpaks influence copyleft, GPL and LGPL licenses. It is a common misconception that these new packaging formats significantly influence license compliance.\nDisclaimer: I am not a lawyer, this is not legal advice.\nDistributing your software in a Snap has no effect on GPL copyleft and LGPL compliance.\n It has no effect on copyleft because containers do not trigger copyleft according to the GNU GPL FAQ [see note]. It has no effect on LGPL compliance because the user can modify the LGPL library and relink the application using LD_LIBRARY_PATH or by unsquashing the snap.  If your software is compliant when running outside of a snap, it will still be compliant when running inside of a snap.\nLonger explanation Two questions arise about GPL licensing and snaps.\nWhen my snap includes GPL software, should everything in the snap be released as open source? No.\nWhen you add copyleft code to your application, you need to open source your entire application. The GPL is the most widely-used copyleft license and it uses the term combined work to describe an application which includes GPL code. The entire combined work needs to be open source, even if only a small part is GPL.\nThe GPL clearly makes the distinction between a \u0026ldquo;combined work\u0026rdquo; and an “aggregate”:\n If GPL code is part of a combined work, all parts of that combined work need to be open sourced. If GPL code is \u0026ldquo;aggregated\u0026rdquo; together with other software, the other software does not need to be open sourced.  So the question becomes \u0026ldquo;is a snap a combined work or a mere aggregation?\u0026rdquo;. The GPL FAQ is quite clear that containers, such as Snaps or Flatpaks, do not create combined works [see note]:\n Q: When it comes to determining whether two pieces of software form a single work, does the fact that the code is in one or more containers have any effect?\nA: No, the analysis of whether they are a single work or an aggregate is unchanged by the involvement of containers.\n This works in both ways: if your application is a combined work outside of a container, it will be a combined work inside of a container. If your application is an aggregate outside of a container, it will be so inside of a container. See What is the difference between an “aggregate” and other kinds of “modified versions”? for more information.\nWhen my snapped application uses an LGPL library, should the application be released as open source? No.\nClosed source application are allowed to link to LGPL libraries as long as a number of requirements are met. The GNU GPL FAQ explains the one most relevant to snaps [see note]:\n For the purpose of complying with the LGPL (any extant version: v2, v2.1 or v3):\n(1) If you statically link against an LGPLed library, you must also provide your application in an object (not necessarily source) format, so that a user has the opportunity to modify the library and relink the application.\n(2) If you dynamically link against an LGPLed library already present on the user\u0026rsquo;s computer, you need not convey the library\u0026rsquo;s source. On the other hand, if you yourself convey the executable LGPLed library along with your application, whether linked with statically or dynamically, you must also convey the library\u0026rsquo;s sources, in one of the ways for which the LGPL provides.\n When you put an application which uses dynamically linked libraries in a snap, the dynamic libraries are distributed in the same snap, but they are not linked yet. When your application starts, the libraries are dynamically linked to your application, just as they would outside of a snap.\nSome might argue that a snap is statically linked because every library is in the same file. However, even in that case, your application is still compliant because the snap provides the application in an object format so that a user has the opportunity to modify the library and relink the application: the individual binaries and libraries of the snap are available in the filesystem at /snap/\u0026lt;snap-name\u0026gt;/\u0026lt;revision/ or by unsquashing the snap manually, and users can relink applications using LD_LIBRARY_PATH.\nNote I use the FAQ as an authoritative source because it holds legal value and the language is a lot clearer than the licenses themselves.\nIn license infringement cases, the spirit of the license holds value. This means what the creators of a license wanted to achieve with it, is important. The GNU GPL FAQ is written by the creators of the GPL license and explains what their intention was. Because of this, the FAQ can be used in court to explain what the license means.\n Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-07-17-lgpl-copyleft-snaps/"
  },{
    "title": "Why Linux desktop apps need containers",
    "date": "",
    "description": "",
    "body": "The Snap Store and Flathub are two universal app stores for Linux. They are very different from how traditional software distribution works. As is always the case with new software, the question \u0026ldquo;why do we need this?\u0026rdquo; often arises. \u0026ldquo;Including software in distribution repositories has worked for so long, so why do we need to change it?\u0026rdquo;\n Note: for an introduction into Flatpak and Snap, please read The future of Linux desktop application delivery is Flatpak and Snap.\n There are many reasons we need Snaps and Flatpaks, but this post will discuss two:\n We need it for software that is not developed at the same speed as distributions. We need it for software that is too \u0026ldquo;messy\u0026rdquo; for distributions.  Let\u0026rsquo;s look at some examples based on snap packages I\u0026rsquo;m maintaining.\nApps which release too quickly Distributions such as Ubuntu and Fedora release a regular version about twice a year. The software available to that version changes very little in between releases, so users often have to wait six months to get major updates to an app. To make matters worse, users of the Ubuntu LTS version have to wait two years for major updates.\nIn contrast, the Arduino IDE sometimes releases a new version after just one month. New versions of the Arduino IDE add support for new hardware and software, so it\u0026rsquo;s important for users to get the latest version as soon as possible. The Arduino IDE Snap updates immediately when a new version is released because the app and its dependencies are completely isolated from the OS itself.\nApps which release too slowly \u0026ldquo;Scratch for Arduino (S4A)\u0026rdquo; gives kids an introduction into robotics by allowing them to program robots using scratch. Although this application is still used for a lot for STEM introductions, it uses very old libraries, making it very hard to install on a recent Linux distribution. The application is developed by a group of educators as a side project and they do not have the resources to update S4A every six months so it works with the newest Ubuntu libraries. The snap of Scratch for Arduino solves this issue by including set of older 32-bit libraries into the snap. These libraries still receive security fixes, but it is very hard to install them on newer Ubuntu releases. Thanks to the snap container, this piece of software can still be used on newer Ubuntu releases.\nApps which are too messy The Bridge Designer is used in middle- and high-schools to give kids a realistic introduction to engineering. The kids get to build, design and test a bridge, similar to how actual engineers do it. The problem with this application is that it is actually a Windows application. Windows software has no place in the Debian repositories. It is however possible to run the software on Linux using a specially crafted Wine environment, which is exactly what the Bridge Designer snap does. Thanks to this snap, users can now install Bridge Designer on Linux without having to mess with Wine.\nMyth: it\u0026rsquo;s only for closed source software All these applications are open source. It is a common myth that these kind of issues only happen in closed source software, but this is clearly not the case. Being open source does not mean a project has the resources to be ported to Linux and it does not mean the developers have enough time to update their software every six months.\nExtra 1: Arduino is also too messy Next to the \u0026ldquo;speed\u0026rdquo; issue, the Arduino IDE has not been updated in Debian and Ubuntu since 2015 because of licensing issues. Arduino source code uses a number of different open source licenses and according to Debian rules, it is not clear enough which files use which license. Although I think this issue should be fixed, Ubuntu users still need the Arduino IDE in the meantime. Because the snap is completely self-contained and does not affect any other part of the OS, it does not need to follow the strict rules of Debian.\nExtra 2: Alternatives Since these issues are universal, people have come up with other very smart solutions.\n AppImage solves these issues by creating a \u0026ldquo;fat binary\u0026rdquo; for each application. Each binary contains all the dependencies an application needs. One advantage is that an AppImage runs by itself; you do not need an additional package manager to run an app. Nix is a package manager which allows multiple installed apps to use different versions of the same libraries. One advantage over Snaps and Flatpaks is that applications generally use less space because shared libraries are not actually duplicated. \u0026ldquo;Rolling\u0026rdquo; distributions such as Manjaro do not release every X months, but continuously releases updates at the same pace of the actual developers of the software. This solves the \u0026ldquo;too fast\u0026rdquo; apps, but not the other issues. This is one of the reasons Manjaro also includes Snapd and Flatpak by default.   Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-07-06-why-snap-flatpak/"
  },{
    "title": "A fundamental difference between Snap and Flatpak",
    "date": "",
    "description": "",
    "body": "Snaps and Flatpaks are often compared to each other because they both make it super easy for Linux users to get the latest versions of desktop applications. If a Linux user wants to install the latest version of apps like Slack, Krita or Blender, either tool will work just fine. There is one fundamental difference between Snaps and Flatpaks, however. While both are systems for distributing Linux apps, snap is also a tool to build Linux Distributions.\n Note: Snap and Flatpak are the software behind two universal Linux app stores: the Snap Store and Flathub. For an introduction, please read The future of Linux desktop application delivery is Flatpak and Snap.\n Flatpak is designed to install and update \u0026ldquo;apps\u0026rdquo;; user-facing software such as video editors, chat programs and more. Your operating system, however, contains a lot more software than apps. It contains a kernel, printer drivers, audio subsystems and more. While Flatpak assumes this software is installed using a traditional package manager, snaps can install anything. These are some examples.\n There is current work ongoing to put the entire Linux printing stack inside of a snap. This has the advantage that printer drivers can be updated independently from the operating system. Once this work is complete, every single Ubuntu version will be able to use the latest printer drivers. Trying to use new printers on old Linux distributions can be very frustrating, and installing newer printer drivers can be risky. Having the printing stack in a snap will solve this issue. A few years ago, Ubuntu drastically changed the system theme. When this initiative started, we wanted an easy way to make the latest updates of the theme available to users immediately. Normally, a system theme is shipped together with the distro, so users do not get theme updates after the distro releases. For \u0026ldquo;CommuniTheme\u0026rdquo;, however, we fixed this by putting the system theme inside of a snap. Because of this, users got updates to their theme every day, instead of every 6 months. This is again not something Flatpak was built for. Flatpak applications can update their own theme, but it is not possible to ship the system theme as a Flatpak. This is because Flatpak was designed for distributing apps, not building an entire Linux distribution. Even the Linux kernel, the most fundamental part of a Linux distribution, can be put in a snap. This is used a lot for IoT devices such as routers and satellites. The impact of a broken kernel update is catastrophic if you require a rocket in order to plug a USB stick into the device. Snaps allow these devices to safely update their kernel and automatically roll back if something goes wrong during the process.  As a result, it\u0026rsquo;s possible to build an entire operating system using only snaps, which is exactly what Ubuntu Core is.\nJust to be clear, this is not meant to be criticism of Flatpak. Flatpak was designed to give developers an easy way to bring their apps directly to users, and it does that job very well. The focussed approach of Flatpak even has a big advantage: it\u0026rsquo;s a lot easier for a distribution to integrate with Flatpak because it does a lot less. The tradeoff is that it only provides app distribution; it doesn\u0026rsquo;t solve the issues of distributing entire operating systems. Fedora Silverblue, for example, creates an immutable desktop operating system by using Flatpak for app distribution and OSTree for distributing the OS itself.\n Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-07-03-snap-vs-flatpak/"
  },{
    "title": "Tutorial: S4A on Ubuntu",
    "date": "",
    "description": "",
    "body": "S4A, or Scratch for Arduino, is an app that you can use to program Arduino boards using Scratch. You can program robots and LED\u0026rsquo;s without writing any code: all you need to do is drag and drop instructions in the visual programming environment.\nStep 1: Install Arduino IDE and S4A The first step is to install Arduino and S4A from the Snap Store.\nsudo snap install arduino s4a After that, you need to give the user account on your computer access to USB devices. Only administrators and user in the dialout group have access to Arduino board connected over USB. Because of that, you will get a \u0026ldquo;Permission Denied\u0026rdquo; error when you try to upload a program. In order to give your user access, run the following command and restart your computer.\n# Add the current user to the `dailout` group sudo usermod -a -G dialout $USER After this, you need to restart your computer.\nStep 2: Upload S4A firmware to Arduino board The next step is to Flash the S4A firmware on your Arduino board. This is required so that S4A can connect to your board to run your apps on it. You need to install this firmware using the Arduino IDE.\n  Download the S4A firmware from here and open it in the Arduino IDE.\n  Connect your arduino UNO board to your computer using USB.\n  Upload the firmware to your arduino by clicking the right arrow at the top of the Arduino IDE. When the firmware is uploaded you\u0026rsquo;ll see the text \u0026ldquo;Done Uploading.\u0026rdquo; on the grey bar just below the code.\n  Step 3: Use S4A After the upload succeeds, you can open the S4A program and start programming your board!\nFrequently Asked Questions When I upload code to the Arduino, I get a \u0026ldquo;permission denied\u0026rdquo; error When you get the following error, that means that the Arduino IDE is not allowed access to the Arduino board.\nAn error occurred while uploading the sketch avrdude: ser_open(): can't open device \u0026quot;/dev/ttyUSB0\u0026quot;: Permission denied This means that either the IDE or your current user is not allowed access to the Arduino USB device. Make sure you execute the following commands as the user which runs the IDE.\n# Give the current user access to the USB device. sudo usermod -a -G dialout $USER After this, restart your computer and try again.\n Note: if the user which runs the IDE does not have sudo permissions, you can grant the user permissions from another account by running sudo usermod -a -G dialout \u0026lt;username\u0026gt; and replacing \u0026lt;username\u0026gt; to the name of the user running the Arduino IDE.\n ",
    "ref": "/blog/2020-02-s4a-setup-ubuntu/"
  },{
    "title": "What's up with CRI-O, Kata Containers and Podman?",
    "date": "",
    "description": "",
    "body": "The container ecosystem is moving rapidly. A lot happened in 2019! It seems the container ecosystem has arrived at the \u0026ldquo;consolidation\u0026rdquo; stage of the hype cycle:\n Docker Inc. sold off its enterprise business and had a bunch of restructuring. They shelved Docker Swarm saying \u0026ldquo;The primary orchestrator going forward is Kubernetes.\u0026rdquo; Twitter joined Spotify in moving away from Mesos-based container orchestration to Kubernetes. This was a huge blow to Mesos given Twitter was its original developer. The Cloud Native Compute Foundation archived rkt. Canonical doubled down on Kubernetes with MicroKubernetes for local development and Charmed Kubernetes for production setups and introduced support for Kata Containers to improve the security and isolation of containers. Red Hat released RHEL8 without Docker support, pointing customers towards their own Podman and CRI-O project.  Kubernetes is the clear winner here, but how do Podman, CRI-O and Kata Containers relate to this ecosystem? The diagram below gives a simplified overview.\nMore explanation below.\nStandards and organizations  The OCI or Open Containers Initiative is an organization that creates container standards. The OCI runtime spec defines the API of a low-level container runtime and the OCI image spec defines what a \u0026ldquo;Docker image\u0026rdquo; actually is. The CNCF is the Cloud Native Compute Foundation, an organization created by the Linux Foundation which hosts a big number of open source projects that all have to do with running \u0026ldquo;cloud native\u0026rdquo; applications. This is mostly about microservices and containers. The Kubernetes project has also defined a number of standards. Relevant for this article is the CRI: the Container Runtime Interface. This interface defines how Kubernetes talks with a high-level container runtime.  But what is a high-level container runtime exactly? First; we\u0026rsquo;ll look at what a low-level container runtime does.\nHigh- vs Low-level container runtime An OCI runtime is relatively simple. You give it the root filesystem of the container and a json file describing core properties of the container, and the runtime spins up the container and connects it to an existing network using a pre-start hook.\nSo what an OCI runtime does not do is the following.\n Actually creating the network of a container. Managing container images. Preparing the environment of a container. Managing local/persistent storage.  All this is the job of a high-level container runtime. On top of this, the high-level container runtime implements the CRI so that Kubernetes has an easy way to drive the runtime.\nAt the moment, we have three main OCI runtimes or low-level container runtimes.\n runc, which is the default for most tools such as Docker and Podman. This is based on the code initially donated by Docker. kata-run from the \u0026ldquo;Kata Containers\u0026rdquo; project, which aims to provide much better security and isolation between containers by running each container in a lightweight VM. It\u0026rsquo;s a merge of the runv and Intel Clear Containers projects. gVisor is created by Google. It provides better isolation by running each container in a tight security sandbox.  There are also three main high-level container runtimes.\n containerd is a CRI-compatible container runtime which was donated to the CNCF by Docker. It is currently the default in many Kubernetes distributions such ad Canonical\u0026rsquo;s Charmed Kubernetes. It supports all OCI-compliant runtimes and has a special shim for kata-run. CRI-O is a bridge between Kubernetes and OCI-compliant runtimes created by Red Hat. It has the big advantage that it gets released in lock-step with Kubernetes itself. Each CRI-O version is compatible with the Kubernetes version that has the same version number. This runtime is the default in OpenShift. Docker itself can also be used as a CIR-compatible container runtime using the docker-shim. However, many Kubernetes distributors are moving away from this solution, due to the added unnecessary complexity of Docker.  Podman, Buildah and crictl Now, what was this about Red Hat moving from Docker to Podman in RHEL8? Scott McCarty wrote an excellent blog series excplaining Why There Is No Docker in OpenShift 4 and RHEL 8. In short:\n Docker is a big fat daemon running almost always as root and which does a million things in the same binary. He argues it\u0026rsquo;s better to split up functionality in a bunch of different projects so you can pick and choose which ones you actually need. Development of the Docker engine is too slow to keep up with Kubernetes. Docker\u0026rsquo;s community is on life support because of how Docker handled the CE/EE split and the move/rename to the Moby project. Docker Inc.\u0026rsquo;s future is uncertain.  So, what should we use instead of Docker?\n Use Podman managing pods and containers. It\u0026rsquo;s a CLI tool which is very similar to docker. It uses libpod which uses runc in backend and is fully compatible with \u0026ldquo;Docker Images\u0026rdquo;. Use Buildah for building \u0026ldquo;Docker Images\u0026rdquo;. It supports building containers from DockerFiles, but you can also build them with simple shell scripts! Use CRI-O for running containers with Kubernetes. If you want to debug pods and containers maintained by Kubernetes, you can use the crictl tool instead of the docker commands.  Conclusion Developers who use Docker and Kubernetes might have been surprised that Docker Inc. is in such bad shape. Further inspection, however, shows an entire ecosystem that is moving on. Docker clearly started this entire revolution, but it just didn\u0026rsquo;t work out. The restructured Docker inc. will focus solely on developers. This might turn out to be an excellent move, since the docker client is still the prime way for developers to build run \u0026ldquo;Docker containers\u0026rdquo; on their development workstations. It will be interesting to watch this space in 2020 and see if Docker is able to implement this vision.\n A special thanks to Kunal Kushwaha for his excellent presentation on Kubecon \u0026ldquo;How Container Runtimes matter in Kubernetes?\u0026quot;. Container ship image by Sergii ILyushin  ",
    "ref": "/blog/2020-01-docker-podman-kata-cri-o/"
  },{
    "title": "How I became a cyborg pt1",
    "date": "",
    "description": "",
    "body": "Close Encounters of the Third Kind \u0026ldquo;We really need to do something about the valve. We\u0026rsquo;ve passed the point where we can wait two more years to see how it progresses.\u0026rdquo;\nI was silent for a while, so the cardiologist asked \u0026ldquo;do you understand what I mean?\u0026rdquo;\n\u0026ldquo;Like, I need a transplant from a pig heart?\u0026rdquo;, I asked.\n\u0026ldquo;Well, given your age an artificial valve is more likely.\u0026rdquo;\nIt was clear my heart was fucked. I knew it wasn\u0026rsquo;t a good sign when the doctor sounded more and more worried as she looked at my heart, and hurried in colleagues to get a second and third opinion. Words like \u0026ldquo;bulging\u0026rdquo; and \u0026ldquo;hypertrophy\u0026rdquo; didn\u0026rsquo;t mean much to to me and I couldn\u0026rsquo;t really ask for clarification since they were looking at my heart through a gigantic tube stuck down my throat. After the procedure, the doctor didn\u0026rsquo;t want to discuss the results with me, saying that the cardiologist would explain everything. She just assured me that \u0026ldquo;everything will work out fine\u0026rdquo;; implying that things were, in fact, not fine.\nThe cardiologist revealed the severity of the issue. Normal hearts have a bunch of valves that make sure blood can only flow in one direction. The aorta valve specifically closes off your heart after each heartbeat to make sure the blood flows to your organs instead of back into your heart. My aorta valve, however, missed that memo. It leaked. Like a lot. Probably a birth defect. My heart took this like a boss for most of my life and just compensated the fuck out of it, but it was starting to take its toll; my heart was becoming enlarged.\n  Frontal image of the bad valve. This should normally look like a Mercedes logo; showing the three leaflets of the valve. However, only the top-left leaflet of this valve is regular. The two others are fused together and badly damaged.    Side view of the valve when it\u0026#39;s closed with my heart on the left and the aorta on the right. The brightly colored stream on the left is blood going the wrong way; it flows back in to my heart, leaking through the closed valve.  Despite this issue, the cardiologist was also optimistic.\n\u0026ldquo;The good news is that, because we found it so early, your heart should be able to recover after we fix the valve. However, really need to do something about it in time. What do you have planned the following weeks?\u0026rdquo;\nI have to admit that 95% of my brain was focused on how his definition of \u0026ldquo;in time\u0026rdquo; was wildly different from mine. \u0026ldquo;umm, I have some time available, I think..\u0026rdquo;\n\u0026ldquo;Great! You can discuss the details with the secretary.\u0026rdquo;\nBefore I knew it, the secretary was calling different departments to schedule a bunch of preparatory exams. Open Heart surgery is pretty gruelling to your body so they wanted to make sure I didn\u0026rsquo;t have any other issues. There was a lot that needed to be done and my brain jumped on it like any other project. Emotionally detached, as if it was a complicated process to fix my computer. I scheduled the appointments, read up on the illness and the procedures and informed family and friends.\nThere was still some uncertainty about the extent of the issue. The issue with my valve was congenital; it just didn\u0026rsquo;t get the memo on how to do this \u0026ldquo;valve\u0026rdquo; thing properly and the doctors wanted to see if any other parts of my heart and veins were slacking off. They did this using a coronarography, or \u0026ldquo;an alien\u0026rsquo;s wet dream\u0026rdquo;, as I like to call it. They opened two veins in my groin, put in long probes to reach my heart, injected a bunch of contrast fluid, and watched it flow using a Rontgen scan. To top it all off, this procedure was done while I was awake and I could follow along on a giant screen next to me. If this sound uncomfortable, that\u0026rsquo;s probably because it is. Thank god I live in Belgium; just imagine living in the US and becoming financially ruined while some doctors reenact the final scene of \u0026ldquo;the fourth kind\u0026rdquo; on you..\nThankfully, the result of the coronarography was positive: the other parts of my heart were nicely executing their assigned tasks, and the surgeons were once again paid to probe a fellow human being. Even better, I now had a bunch of cool videos showing how my heart (mal)functioned.\n  --  Your browser does not support the video tag.  This video shows a probe injecting contrast fluid in my aorta. Most of the fluid is pumped up through the aorta to the rest of the body. However, due to the leaking valve, part of the fluid is sucked down, back into the left ventricle of the heart itself.\n Next step: the heart surgery itself! We scheduled it about six weeks after the coronarography so that I had some time to finish up at work and go on a holiday. However, some complications foiled the plan of finishing up at work. All the probing in my groin tore one of my veins, causing a pseudo-aneurism. After a week of trying to fix it from the outside, the doctors gave up and did a surgery under full anesthesia to sew everything back together. Long story short; I ended up losing four of those six weeks to recovering from the coronarography. Thankfully, the doctor cleared me to go on a holiday after that.\nHow I became a cyborg pt2\n",
    "ref": "/blog/2019-steampunk-cyborg-1/"
  },{
    "title": "Who owns my research?",
    "date": "",
    "description": "",
    "body": "When I submitted my first paper, I realized I had no idea who owned my research! Was I authorized to transfer the rights to a journal or did I need someone else\u0026rsquo;s permission? After some digging, I found the answer and wrote it all up in a neat little document. So here\u0026rsquo;s the dusted off version, in case it\u0026rsquo;s useful to anyone else.\nIn short Intellectual property (IP) as a PhD student at Ghent University is largely divided into two parts: research results and educational material.\n The IP on research results are automatically transferred to the university, except for literary and artistic works. So you own the papers you write and the university owns the patents you create. The software and databases you create are also owned by the university. This is an exception to the general rule since Software is generally seen as a literary or artistic work. The IP on the educational material you create is completely owned by you, except for software and databases. So the course materials and educational books you write are completely owned by you. As a result, your students need to have your permission to copy and sell your course materials.  a) Research Results Note that different rules might apply if your research is in collaboration with a company or another research institution.\nGhent University Education and Examination Code Article 85 of the Ghent University Education and Examination Code deals with IP on research results.\n §1. In execution of article 4 of the General Research and Co-operation Regulations of Ghent University Association (Algemeen Onderzoeks- en Samenwerkingsreglement van de Associatie Universiteit Gent, AOSR), all doctoral students who are considered voluntary researchers in accordance with article IV.48 of the Codex Higher Education transfer all property rights of their research results to Ghent University upon enrolment. Unless stated otherwise in their contract, all doctoral students are treated as researchers at Ghent University as far as the implementation is concerned of all applicable regulations on the valorization of research results.\n§2. The supervisor(s) and the doctoral students see to it that all research results that can create value are reported to the Technology Transfer Office prior to publication in any which shape or form, in accordance with the AOSR.\n The document also explains what \u0026ldquo;research results\u0026rdquo; are.\n The results of research or development efforts, accomplished by the researchers as part of their relation with Ghent University and/or by means of Ghent University resources or equipment. These do not comprise literary works or works of art as intended under the Law on Copyright and Neighbouring Rights of 30 June 1994 (‘Copyright Act’). However, they are considered to comprise computer software or databases which are protected under the Copyright Act and/or the Act of 31 August 1998 which transposed into Belgian law the European Directive of 11 March 1996 on the Legal Protection of Databases (‘Database Act’).\n Higher Education Codex The Higher Education Codex is a Flemish decree governing the higher education in Flanders. It is sadly not available in English, so all translations here are of my own.\nArticle IV.48.. explains that the university own the IP rights on the research results of its personnel and researchers associated with the university. \u0026ldquo;Research results\u0026rdquo; is defined as potentially patentable inventions, (biological) culture products, drawings and models, semiconductor topographies, computer programs and databases that are useable for industrial or agricultural industrial purposes.\nNote that this excludes literary works.\nUgent Valorisation Regulation The Ghent University Valorisation Regulation also talks about the IP of research results. It is sadly also not available in English. It defines what \u0026ldquo;research results\u0026rdquo; as \u0026ldquo;results of research and development that researchers create as part of their relationship with Ghent University and/or using resources of Ghent University\u0026rdquo;. It explicitly says software and databases are part of research results, but literary works and works of art are not.\n De Onderzoeksresultaten zijn de resultaten van onderzoek of ontwikkeling die door de Onderzoekers worden gerealiseerd in het kader van hun relatie met de Universiteit en/of dankzij het gebruik van de universitaire middelen of uitrusting. Werken van letterkunde of kunst in de zin van Boek XI, Titel 5 van het Wetboek van Economisch Recht (de “auteurswet”) maken voor toepassing van onderhavig reglement geen deel uit van de Onderzoeksresultaten. Computerprogramma\u0026rsquo;s of databanken beschermd krachtens Boek XI, Titel 6 en 7 van het Wetboek van Economisch Recht maken wel deel uit van de Onderzoeksresultaten. Onderzoeksresultaten behaald door een student, bijvoorbeeld in het kader van een afstudeerwerk, behoren ook tot de Onderzoeksresultaten zoals hier gedefinieerd in de mate dat bij de creatie van resultaten van de student beroep gedaan wordt op universitaire middelen.\n\u0026hellip;\nARTIKEL 2 VERMOGENSRECHTEN OP ONDERZOEKSRESULTATEN 2.1 De vermogensrechten op de Onderzoeksresultaten komen toe aan de Universiteit Gent als rechtspersoon, behoudens andersluidende bepalingen in reglementeringen en/of overeenkomsten goedgekeurd door het Universiteitsbestuur. 2.2 De Universiteit eerbiedigt de morele rechten, inclusief het vaderschapsrecht, die uitsluitend eigendom blijven van de Onderzoekers.\n imec Confidentiality Agreement Researchers at the IDLab research group at Ghent University were asked to personally sign a confidentiality agreement with imec. Since this agreement itself is not publicly available, I will not talk about it further, but you should read it if it applies to you.\nb) Educational Materials Neither the Ghent University Education and Examination Code nor the Ghent University Valorisation Regulation talk about IP rights on educational materials created by employees. As a result, the belgian copyright law applies here.\nThe Belgian copyright law states that creators of a work own the IP of that work by default, even if it is made as part of their job. The law goes on to say that employment contracts can state that copyright is automatically assigned to the employer in certain conditions.\n Auteursrechten ontstaan bij de maker van een werk. Het zal dus de arbeider, bediende of personeelslid onder statuut zijn, en niet de werkgever, die de rechten verwerft op een werk dat door hem is gemaakt in het kader van een arbeidsbetrekking. Deze regel is daarentegen niet van toepassing voor computerprogramma’s, databanken en tekeningen en modellen.\n\u0026hellip;\nEen overeenkomst tot overdracht van auteursrechten, zoals hierboven beschreven, zal slechts geldig zijn met betrekking tot de werken die de werknemer heeft gemaakt in het kader van zijn arbeidsovereenkomst of de hem toevertrouwde opdracht.\n The valorisation department of Ghent University confirmed that this is the case; the course materials you create are owned by you.\nIf you contact the department of valorisation of Ghent University, they will confirm that this is the case: the course materials you create are owned by you.\nMaak je een cursus in opdracht van de school, dan wordt de school automatisch eigenaar en heeft er dus auteursrechten over. Ontwerp je op eigen initiatief een cursus, dan heb jij de auteursrechten. * [Source](https://onderwijs.vlaanderen.be/nl/auteursrechten-op-school#Zelfgemaaktecursus) * [Source](http://anspire.be/auteursrechten/download/Lespakket%20auteursrechten%20en%20licenties%20-%20v2.pdf) -- ",
    "ref": "/blog/copyright-ghent-university-researcher/"
  },{
    "title": "I'm Back!",
    "date": "",
    "description": "",
    "body": "I restored my blog from an old backup I had. I imported it into Hugo, a static blogging platform.\nImage by clipart-library\n",
    "ref": "/blog/2019-05-im-back/"
  },{
    "title": "What does “perf interrupt took too long” mean?",
    "date": "",
    "description": "",
    "body": "Wifi problems So, I\u0026rsquo;m having a problem with my laptop. When I\u0026rsquo;m using the “TelenetWiFree” connection, I get disconnected after a certain amount of time, and for some reason I cannot reconnect until I restart my computer. Toggling the hardware Wifi kill-switch, which lets you disable and enable the power to the wifi hardware, does not resolve the problem. After re-enabling the hardware, the wifi doesn\u0026rsquo;t seem to come back again. Only a reboot makes the Wifi work again…\nThe “TelenetWiFree” Wifi doesn\u0026rsquo;t play well with all the computers we have, three Ubuntu laptops and one Chromebook, but it seems particularly wonky with my computer. I had some free time, so I started digging into the problem.\nWho watches the watchmen? It seems the kernel does… In the dmesg output, I found the following line:\n perf interrupt took too long (2528 \u0026gt; 2500), lowering kernel.perf_event_max_sample_rate to 50000\n This doesn\u0026rsquo;t seem right, maybe this is the source of the problem! Let\u0026rsquo;s do some digging:\n Perf is a Linux Kernel performance monitor. Perf uses Non-Maskable Interrupts. This basically means Perf can tell the cpu “Pauze whatever you\u0026rsquo;re doing now and let me do something”. This allows perf to have a “watchdog” functionality where it can monitor even the most critical processes and interrupts.  With great power comes great responsibility. Perf can hang your computer by constantly pauzing everything. To make sure this won\u0026rsquo;t happen, the kernel itself monitors perf. When it decides perf is pauzing too long, it tells perf to do a little bit less. That\u0026rsquo;s basically what that dmesg line means. The kernel told perf to do a little bit less. Does it have anything to do with the Wifi problem? Probably not… But at least I learned something: The kernel watches the watchmen…\nAdditional sources:\n https://superuser.com/questions/757444/perf-samples-too-long-lowering-kernel-perf-event-max-sample-rate-to https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1246216 https://bbs.archlinux.org/viewtopic.php?id=170471 http://ubuntuforums.org/showthread.php?t=2253552  Image by David Masters\n",
    "ref": "/blog/2015-09-perf-interrupt-took-to-long/"
  },{
    "title": "Pragmatic Docker Day 2015",
    "date": "",
    "description": "",
    "body": "I got a ticket to the Pragmatic Docker Day meetup in Ghent in exchange for writing a blogpost about it. Free food, drinks \u0026amp; awesome talks for a whole day, who would want to miss that? Not me!\nSo here are my unstructured thoughts I gathered from the meetup:\nDocker is awesome! What is docker? Docker encapsulates apps in their own little sandbox. Every app runs in its own container. An environment made especially so the app can run well and can\u0026rsquo;t mess to much with other apps running on the same server. It\u0026rsquo;s basically a VM without the overhead.\n Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications\n Developers write their apps and put them in a docker container. They give this container to sysadmins and they are sure everything will work once deployed on the server because the app and all its dependencies are bundled in the docker container.\nDocker is fancy! This isn\u0026rsquo;t new at all. Containers have existed on Linux for a very long time. What docker does is provide a very fancy interface and API to work with and distribute those containers. This opens container technologies to a much broader public.\nDocker is becoming so popular that even Microsoft wants in on the party. Docker announced a partnership with Microsoft to bring Docker to Windows.\nDocker is production-ready* Tomas Doran talked about how Yelp is using Docker in their Continues Integration \u0026amp; Continues Deployment workflow. From what I gathered, almost every service at Yelp is running in docker containers. This allows for very dynamic scaling, and makes the development – testing – production chain a lot shorter without compromising on the stability.\n*Docker is still a very young and fast-moving platform. Using this in production requires a lot of knowledge and requires some caution. For a small infrastructure the benefits you get from docker might not outweigh the disadvantages from working with such early technology. However, even when you don\u0026rsquo;t use Docker in production, you can still benefit from it during development and testing!\nDocker containers are not cross-platform… The docker container still uses the host\u0026rsquo;s kernel. This means that a docker container will only work on a machine that supports the same kernel functionality. If you make a docker container on Linux, and you run it on a distro with a different version of the Linux kernel, most of the time you\u0026rsquo;ll be allright. However, running Linux docker containers native on a Mac or Windows won\u0026rsquo;t work.\nYou can still run containers on Windows?! It\u0026rsquo;s still possible to run Linux docker containers on Mac and Windows using boot2docker. This is basically a lightweight Linux VM. Docker containers run in this VM. This is great for development, but there might be an overhead because of the virtualization.\nImage by distel2610\n",
    "ref": "/blog/2015-04-pragmatic-docker-day/"
  },{
    "title": "Windows power users are not Linux power users!",
    "date": "",
    "description": "",
    "body": "Let\u0026rsquo;s rid the world of one of the most prominent Linux myths: “Desktop Linux sucks. It\u0026rsquo;s just not there yet”.\nWhen Windows power users try out Linux two things always happen:\n They get frustrated because they can\u0026rsquo;t find how to do something in Linux. They brick their system, or make it extremely buggy.  I\u0026rsquo;ve been that person, I\u0026rsquo;ve bricked my system (a lot!), and the fact is, it was entirely my own fault. Linux didn\u0026rsquo;t suck, I made it suck! This post is a summary of every mistake I made the last 4 years, in an attempt to prevent other people from making the same ones.\n  Let\u0026#39;s rid the world of a few Linux misconceptions  Linux is different Let\u0026rsquo;s start with an example: Bill is trying out Ubuntu and wants to change his screen resolution. Bill knows that on Windows, you can right-click the desktop background and click “change screen resolution”. Bill tries this, but cannot find the “change screen resolution” option. Bill can now do two things:\n [the bad way] Assume you can only change this using the commandline. Go online and rant about how Ubuntu is impossibly hard to use. [the good way] Open the dash and search for “Resolution” or “screen” or “display” or “monitor” or “projector” or whatever. He will find the “Displays” application. If he isn\u0026rsquo;t entirely sure if that is the right application, he can right-click the icon and he will see a description of what the application does.  Linux is not Windows. You are used to doing things a certain way on Windows. Some things will work differently on Linux. You will have to get used to it. This does not mean Linux is hard, only that it is different. Mac has this exact same problem. Windows power users complaining that ctrl-c ctrl-v does not work on a mac, even though the command button makes a lot of sense.\n  And yes, you can change the resolution in a GUI  Windows power user != Linux power user Another example: Jessica knows a thing or two about Windows, she can even re-install Windows, if she finds that dvd she once burned. Jessica want to try out Ubuntu. She takes an old computer lying around and starts up the Ubuntu installer. She knows it\u0026rsquo;s better to have different partitions, so she chooses to partition the disks manually. She tries to configure the partitions, but she keeps getting errors she does not understand. She gets frustrated and rants on G+ about how in Linux, everything is complicated.\nI see this mentality a lot: “I know how to do advanced tasks on Windows. I don\u0026rsquo;t know how to do advanced tasks on Linux, so Linux must be hard.”\nYou can do advanced tasks on Windows, because you learned how to do them on Windows. You will have to learn how to do some of them for Linux. This is the same for every platform. You can be a complete Linux Guru, being able to install Linux From Scratch. But you will still have to relearn how to flash your android phone.\nGive it time, after a while you will come to have the same skill level on Linux as you have on Windows. Just don\u0026rsquo;t expect to get there on the first day.\nYou are a danger to your Linux Exhibit A: Alice her Windows machine is becoming really slow and she wonders what she could do to speed it up a little. Alice googles “how to speed up Windows”. One of the first results is a blog explaining how to use cCleaner to disable processes. Alice, being a Windows power user, knows this can break her system really bad, so she is very careful and googles each process before disabling it. Alice is happy, her Windows is faster again.\nExhibit B: Bob is running the latest Ubuntu on a 10-year old laptop and notices it is a bit slow. Bob is a Windows power user, he figures he knows enough about computers to do something about it. He googles “how to make Ubuntu faster”. He finds a blog telling him to run different commands. The blogpost is a year old and has a lot of “thanks!” comments, so Bob thinks he can trust the author. Bob runs the commands. When a command fails, he figures it is a permission problem, so he runs the command again with “sudo”. The following week, Bob experiences weird glitches and crashes, and his computer cannot connect to his printer anymore. Bob is unhappy and goes on twitter to rage about how buggy Linux is.\nYou\u0026rsquo;re a Windows power user. You can mess with Windows, because you know what is dangerous, you know what warnings you can safely ignore. You are not a Linux power user. You do not know how to make that distinction on Linux, so be very careful!\n  Do not blindly trust commands from the web  I cannot stress this enough. I\u0026rsquo;ve seen this happen so many times, with myself, and with other people. A friend of mine wanted to make a lightweight Ubuntu install for a media center. He was using a heavily outdated guide to do so. The guide instructed him to remove a lot of programs, including compiz. Little did he know that newer versions of Ubuntu(Unity) require compiz to function properly. The result: he bricked his system, and blamed Ubuntu in the process.\nA Desktop environment is not a theme A desktop environment(DE) is a collection of software that does a lot more than just “look good”. A DE handles a lot of the “usability” features of the desktop, like automounting USB-sticks, CD\u0026rsquo;s, DVD\u0026rsquo;s and memory cards. It also helps you set up your network, it configures DHCP, detects wireless networks and gives you a nice user interface to enter the WIFI password. A DE also handles the function keys (brightness, sound, …) on your keyboard, and it can even connect to your smartphone to show you if you have new messages.\nWhen you choose a DE, you basically choose how you will interact with your computer. If you choose a lightweight DE like lxde, you will lose a lot of the out-of-the-box experience a Windows/Mac user might expect. You will have to pop open a terminal, even to do basic stuff, like change your timezone. Unity on the other hand has those features, but demands more from your hardware.\n  Xubuntu might be fast and stable, but it comes with a price  A lot of the DE\u0026rsquo;s share the same libraries and software. Installing multiple DE\u0026rsquo;s at the same time can cause problems. If you want to try out different DE\u0026rsquo;s, I recommend doing a clean install with one of the official Ubuntu flavours, or another distro.\nChoose wisely, and stick to that choice Linux gives you a lot of choice. This is a great strength, but also a weakness. People have a tendency of making bad decisions when presented with so much choice.\nEveryone who tried Linux for the first time has had the same question: “what distro should I use?”. This seems like a very hard question, and the internet gives you a lot of conflicting answers to it. However, for 99% of the people, the answer is really simple.\nIf you\u0026rsquo;re looking for a Window/Mac replacement for your primary machine, and you are new to Linux, you should use default Ubuntu. Ubuntu offers the complete out-of-the-box experience you are used to on Windows/Mac. It is the best supported desktop, and It also has a good community that is very newbie-friendly. Every problem you will have, someone has had already. Askubuntu is full of questions Windows and Mac users might have when switching to Ubuntu.\n  Askubuntu, helping Windows users change to Linux since 2010  You will surely find a DE that is easier to use than Unity. You will also find one that is more stable and one that is more beautiful. However, Unity offers the complete package with very few rough edges.\nIt\u0026rsquo;s also easy to find a Distro that is more up-to-date than Ubuntu. Finding one that is more stable and one that is faster is easy too. But again, Ubuntu offers the complete package, with very few rough edges. It makes the transition very easy and it is a care-free Windows/Mac replacement.\n  With a Numix theme, Unity looks pretty good!  Conclusion Are you thinking of joining the cool kids and trying out Linux? Then start with an easy distro like Ubuntu and be careful with what you do in the commandline. Most important of all, remember that Linux is different, and that\u0026rsquo;s a good thing.\nEnjoy your Linux, and let me know your experiences in the comments down below.\n",
    "ref": "/blog/2014-11-why-you-think-linux-sucks-and-why-its-your-own-fault/"
  }]
