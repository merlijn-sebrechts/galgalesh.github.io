[{
    "title": "How to verify the source of a Snap package",
    "date": "",
    "description": "",
    "body": "Many Snap packages contain two files which allows users to verify what sources were used to build the package.\n snap/snapcraft.yaml is the \u0026ldquo;source\u0026rdquo; of the package. This file was used by snapcraft to build the package. snap/manifest.yaml is a \u0026ldquo;recording\u0026rdquo; of the build of that package. It is similar to the first file, but it includes a lot more information to pinpoint what exact sources were being used. It records the exact package versions of dependencies, the git commit of all source repositories, checksums of any downloaded binary and more.  The manifest is used by the Snap Store to check if Snap packages contain libraries with security vulnerabilities. If this is the case, the publisher gets an email asking them to rebuild the package.\nBoth these files are added by the snapcraft build tool when the environment variable SNAPCRAFT_BUILD_INFO is set during the build. This is automatically enabled for all snaps built by snapcraft.io and Launchpad.\nWhat do the manifests contain? Let\u0026rsquo;s look at Canonical\u0026rsquo;s Chromium snap as an example. If you\u0026rsquo;ve already installed the snap\nIn snap/snapcraft.yaml, source explains what repository is used to build the package and stage-packages describe what dependencies are included in the snap.\ndesktop-gtk3: source: https://github.com/ubuntu/snapcraft-desktop-helpers.git source-subdir: gtk stage-packages: - libxkbcommon0 - ttf-ubuntu-font-family - shared-mime-info ... In snap/manifest.yaml, source-commit is added to narrow down what exact source was used for this package and stage-packages include the exact version of each dependency.\ndesktop-gtk3: source: https://github.com/ubuntu/snapcraft-desktop-helpers.git source-commit: 622e2aa7a840b3a7dbb6ea4d432d687d5cc2e8ef source-subdir: gtk stage-packages: - libxkbcommon0=0.8.2-1~ubuntu18.04.1 - ttf-ubuntu-font-family=1:0.83-2 - shared-mime-info=1.9-2 ... Finally, for snaps built on Launchpad, snap/manifest.yaml also contains a URL to the build log.\nimage-info: build-request-id: lp-58361925 build-request-timestamp: \u0026#39;2020-08-05T11:55:18Z\u0026#39; build_url: https://launchpad.net/~osomon/+snap/chromium-snap-firstrun-notice/+build/1071421 How do I access these files? Currently, you need to download the snaps before you can access the manifest.\n If you don\u0026rsquo;t have the Snap installed, run snap download snap-name, which will download the snap to the current directory. You can then open the snap with any archive manager to look at the files. If you have the snap installed, you can find them in /snap/snap-name/current/snap/.  Can the manifests be doctored? Yes; a malicious publisher can change the contents of these files. If the snap was built on Launchpad or GitHub, however, you can verify if the manifest was changed since the package was built. This is the process to verify a snap build from Launchpad:\n Download the snap and look for the build_url in snap/manifest.yaml. Surf to the URL and download the snap in \u0026ldquo;Built files\u0026rdquo;. Calculate the checksum of both snaps and see if they match.  If the checksums match, you know the manifest was not changed since the package was built. On Launchpad, you can also see which machine the package was built on, the entire build log and the source of that build.\n Note: This process requires you to trust the information in Launchpad is correct. This is the same infrastructure that builds the regular Ubuntu packages and hosts their source code, however, so Ubuntu and all of its derivatives already depend on this information being correct.\n Can I trust any package built on Launchpad? No! It is still very important to trust the publisher of a package themselves. Because the publisher controls the entire source code of the application, it is impossible to stop them from including malicious code. Even if that code is built on Launchpad.\n Note: for this reason, I\u0026rsquo;m not convinced that it\u0026rsquo;s useful to force publishers to include manifests and/or use Launchpad. It would only give users a false sense of security.\n Snap is specifically designed to let developers publish their apps directly to users without a middleman. This also means that users should only install software from developers they trust. The Snap sandbox helps to reduce the damage a malicious or broken snap can do, but it does not completely protect you from harm.\nExtra: What if the Snap was built on GitHub? As James Henstridge pointed out, if the Snap was built on GitHub using the official actions, you can also verify the manifest. The process is a little bit more complicated, though.\n  Download the snap and look for the build_url in snap/manifest.yaml.\n  Surf to the URL go to the log of the \u0026ldquo;build\u0026rdquo; step and look at the Run snapcore/action-publish@v1 part for the \u0026ldquo;revision\u0026rdquo; number.\nRevision 13 of \u0026#39;asciidoc-link-check\u0026#39; created. Track Arch Channel Version Revision latest amd64 stable 1.0.14 13 candidate ‚Üë ‚Üë beta ‚Üë ‚Üë edge 1.0.13 10   Check the following things:\n The name of the snap you just downloaded is the same as in this log. The revision of the snap you just downloaded is the same as in this log. The workflow uses snapcore/action-build and snapcore/action-publish to build and publish the Snap.  If these three are true, then the manifest was not changed since the build because publishers cannot modify the Snap of a revision after it was uploaded. Every change creates a new revision number.\n   Note: This process requires you to trust that the information in the Snap Store and on GitHub is correct.\n Areas of improvement If you\u0026rsquo;re interested in transparency, trust and reproducibility of snaps, there are many areas you can work on to further improve this. From the top of my head, these are some of the things that can improve:\n To see the manifest, users need to download the snap and manually look through the files. It would be very useful if the Snap Store web frontend and the snap CLI command could show you the manifest and the data in it. Although the manifest records everything used to build the package, you can\u0026rsquo;t yet use this recording to reproduce the package. The snapcraft build tool needs to be modified so it can rebuild a snap from the manifest.yaml. You won\u0026rsquo;t be starting from scratch though, as snapcraft already supports many of the fields in the manifest. The manifest currently does not contain the source url of the repository which contains the snapcraft.yaml file itself. You can use Launchpad to find this repository, but it would be better to include this repository in the manifest itself. As far as I know, Launchpad and GitHub are currently the only build services for Snaps which include a link to the build log. It would be useful for others such as the KDE Neon build service to do the same thing.   Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-08-17-verify-snap/"
  },{
    "title": "About Merlijn",
    "date": "",
    "description": "",
    "body": "I\u0026rsquo;m a certified cyborg. I enjoy weird food, hiking, milkshakes and games with a good story. I want to make the world a better place.\nI\u0026rsquo;m a PHD researcher and Teaching Assistant at Ghent University - IDLab - imec.\n My research specializes in DataOps and cloud modelling languages. I assist in teaching distributed data processing, DataOps, DevOps, systems programming, C/C++ and Python.  I\u0026rsquo;m a passionate user of and contributor to open source. Some of the stuff I\u0026rsquo;ve worked on:\n I\u0026rsquo;m maintaining a bunch of Snap packages including Krita, Arduino IDE, Foliate, PhotoScape and Easy OpenVPN Server. I contribute to the snapcraft.io ecosystem by writing documentation, blog posts, and PR\u0026rsquo;s for snapcraft and snapd. I helped kickstart the Ubuntu community theme initiative, though my biggest contribution there might have been suggesting to call it \u0026ldquo;Yaru\u0026rdquo;. I used to be very active in the Juju community, writing a bunch of Big Data Charms as part of the Tengu Team, writing a bunch of documentation, and helping design the 2.0 version of the charms.reactive framework and writing a paper about how cool it is. Had the great pleasure of being appreciated by Jorge Castro and can confirm he is a Chinese botnet. I answered a bunch of questions on AskUbuntu.  I\u0026rsquo;ve been a board member of Hackerspace.Gent since 2014 and wrote the Hackerspace Blueprint; a booklet about how to run a leaderless organization using do-ocracy.\n",
    "ref": "/about/"
  },{
    "title": "How I became a cyborg pt2",
    "date": "",
    "description": "",
    "body": " This is part 2 of a series on how I became a cyborg. Warning: this article talks in details about heart surgery and shows post-op images.\nRead part 1 first\n We can rebuild him 2019-08-06: Waking up from heart surgery.\nThe first thing I saw were my parents talking to a nurse. My parents wanted to leave but the nurse asked them to stay, saying \u0026ldquo;it\u0026rsquo;s better to have familiar faces when he wakes up\u0026rdquo;. I nodded, but they didn\u0026rsquo;t really seem to notice so I just closed my eyes for a bit and waited until I was a bit more awake.\nWhat felt like minutes of drifting in and out of consciousness were actually hours according to my parent\u0026rsquo;s accounts. The more aware I got, the more I felt extremely hot. During surgery, they cooled my body down to 28 ¬∞C to slow my metabolic rate, so after surgery they mummified me in blankets to get my temperature back up but I really don\u0026rsquo;t like being hot.. Given that I could barely move any limb, wiggling my toes loose was a losing battle. Asking my parents for help didn\u0026rsquo;t work either because for some reason I wasn\u0026rsquo;t able to produce any sound. There was nothing left but to use my fingers to try to pull up the blanket but the nurse thwarted this attempt, holding my arm and telling me to lay still because I was still connected to dozens of tubes and sensors. The plus side was that I finally had the attention of the spectators, so I tried to use hand signals to ask them to help my feet escape from their personal sauna. After some frantic pointing they finally understood me and released my feet. Success! Hardest part of the recovery done!\nBy this point, my parents also saw that I tried to speak a few times so they explained to me that I was still intubated. No idea how I could have missed the giant tube going in my lungs. Annoyingly, they didn\u0026rsquo;t answer the single most important question of that moment: \u0026ldquo;How did the surgery go?!\u0026rdquo; Before they put me to sleep, I was under the impressions that I wouldn\u0026rsquo;t be able to see my parents until the day after surgery, so was seeing my parents a good or a bad sign? Were they able to fix the broken valve or did they have to put in a new one? Luckely, the nurse extubated me soon after, allowing me to finally pop the question: \u0026ldquo;So, how did it go?\u0026rdquo;\n\u0026ldquo;Oh?! -as if they were surprised by me aquiring what my current health status was- Everything went well. She wasn\u0026rsquo;t able to fix the valve, but they put in an artificial one and it\u0026rsquo;s working as it should.\u0026rdquo;\nFor some reason, I was kind of hoping for this outcome. Imagine how dumb it must\u0026rsquo;ve been to go through this entire ordeal and not even becoming a cyborg.. At least I had a cool new title now!\nTalking was still difficult for me because I simply couldn\u0026rsquo;t get enough air in my lungs to say a full sentence. I told my parents I was seeing the biggest aura I had ever seen. The nurse gave me a weird look so I explained \u0026ldquo;Like when I\u0026rsquo;m getting a migraine\u0026rdquo;.\n Your browser does not support the video tag.  This is what an aura normally looks like for me. I see this weird flashing shape before I get a migraine attack. It starts with a tiny spot and grows into a big spiral. The headache itself starts a few minutes after the aura disappears.\n The entire top part of my vision was flickering like this for the first few hours. Not that it was a big issue, but I found it very interesting that I had the biggest aura in my life after open heart surgery. It gave me some hope that my migraines were connected to my shitty heart and that they might become better with the new valve..\n üïë Now, a year later, I\u0026rsquo;m a bit disappointed by the effect it had on my headaches. Although they\u0026rsquo;re slightly better, I\u0026rsquo;m still plagued by them.\n After my parents left, the first night started and it was actually a relatively pleasant experience thanks to an amazing nurse who was with me the entire night. I wasn\u0026rsquo;t able to sleep much, though. Every time I fell asleep I seemed to stop breathing and woke back up gasping for air.. During the surgery they gave me medication to stop my breathing reflex. During open heart surgery, they stop your heart and lungs and connect your arteries to a machine which simulates them. For this, you need to stop breathing completely, and apparently it can take a while for everything to normalize again. The nurse was able to comfort me, however, by explaining that I had multiple sensors checking the oxygen level in my blood. She constantly adjusted the extra oxygen I got based on those values and turned the display to me so I could watch along. We talked a lot throughout the first night, which mostly consisted of me asking what every single tube and machine was doing and her explaining that.\n When my parents left, they took a picture to show my family and friends I was alive.\n The pain the first night was relatively ok. The nurse used a whole bunch of pillows to put my upper body in a position that hurt the least, which helped a lot, and the anesthesia was still very much in my body. As the night progressed, the pain got worse, but the nurse kept increasing the painkillers to keep it under control. Even though the pain was under control, I felt as if lying in that bed was one of the most difficult things I\u0026rsquo;ve ever done in my life. The simple act of breathing required a lot of effort. Given my ribs were just cut open, I had to focus on using my stomach to breathe instead of my chest and I had to fight the pain every time I inhaled. Not breathing would have been a much less painful option but most doctors recommend against that. Having to spend so much energy to do something so vital was a very weird experience. As a seemingly healthy 27-year old, I\u0026rsquo;ve never been confronted with the fact that my body can fail to do even the simplest things required to keep me alive. I still felt as if I could take it, though. I still had some fight left in me. I was even making some jokes! I heard my new valve tick and I explained how my childhood dream of becoming a ninja is now shattered because people will be able to hear me tick when I sneak up to them.\n üïë Today, a year in to living with this valve, I\u0026rsquo;m very happy my job doesn\u0026rsquo;t require sneaking around because other people really hear my heart tick when they stand in front of me in a quiet room.\n My mood quickly changed in the morning, however. There is a limit to how much painkillers they can pump through your body. Moreover, because we believed I was allergic to one of the painkillers, the options were even more limited for me. So we reached a point where they couldn\u0026rsquo;t increase the dosage to match my pain level anymore. On top of that, the morning brought in the morning crew, who were surprisingly successful in maintaining a noise level of \u0026ldquo;London underground during rush hour\u0026rdquo;. The morning crew also did an rx scan of my chest and found that part of my right lung collapsed. This is not what you want to hear when you feel like you\u0026rsquo;re only just able to breathe enough to survive.\n Somehow, this picture should show my right lung is collapsed. I can\u0026rsquo;t see it though.. The clamps are from sensors taped to me and the very thin wires going to my heart are for a temporary pace-maker. In the middle you see six tied up metal wires used to hold my ribs closed. You can see my new aorta valve to the right of the fourth wire from the top.\n When I told the nurse I had real difficulties with the noise, she explained they wanted to move me from the ICU to mid-care, which is a lot more quiet, but my blood pressure was still too high to do that. Even though my blood pressure kept lowering throughout the night, for some strange reason it shot back up when the stampede of interns started running through the ICU yelling at each other. The only way to get my blood pressure down was to give me a pill.\nThe pill came with only one side effect: me puking my guts out.\n Hey Merlijn, isn\u0026rsquo;t it very painful to puke your guts out when any chest movement causes shooting pain?\n Good question, attentive reader! This is indeed extremely painful. Furthermore, the relieved feeling you normally get after puking was absent. My body couldn\u0026rsquo;t even pretend like what just happened was useful. Any bit of fight left in my body quickly disappeared. Or in better terms: any bit of fight left in my consciousness quickly disappeared because to my own surprise, my body and mind just kept going even without my support. The orchestra continued playing even though the conductor gave up. I discovered my brain has an auto-pilot ready to take over when my consciousness gave up.\nAfter giving me the same pill again, I was able to hold it in with much effort and they were finally able to wheel me into what felt like the quietest place I have ever been to: the hallway next to the ICU. The road to mid-care was so serene but to my surprise, the mid-care was even better! The lights were even off so I could close my eyes and pretend I could sleep!\nTBC\n",
    "ref": "/blog/2020-steampunk-cyborg-2/"
  },{
    "title": "Why is there only one Snap Store?",
    "date": "",
    "description": "",
    "body": "Snap and Flatpak are the basis of two universal app stores for Linux: the Snap Store and Flathub. Interestingly, Flatpak has multiple repositories: Flathub is the main one but both Fedora and Elementary OS also host their own store. In contrast; there is only one Snap store. Why is that?\n Note: for an introduction into Flatpak and Snap, please read The future of Linux desktop application delivery is Flatpak and Snap.\n Snap is designed so each device only connects to a single store for three reasons:\n users can easily discover new applications, developers can easily publish their apps, and developing Snap itself is easier.  Canonical, the company behind Snap and Ubuntu, already tried the distributed approach and discovered its downsides. Before Snap, \u0026ldquo;Personal Package Archives\u0026rdquo; (PPAs) were the recommended way to get third-party software to Ubuntu users. The idea of these is that each developer creates a tiny repository which includes their app. Users get new software by adding the PPA and installing the software using their package manager. This is very similar to how Flatpak supports multiple \u0026ldquo;stores\u0026rdquo;.\nOn Android and iOS, users often find new software by opening the app store and searching for what they need. This is not possible with PPAs because the software in a PPA only shows up in the Ubuntu app store after a user adds it. Snap solves this by having a single repository with all available apps. Users discover new software straight from the app store instead of having to search the internet.\nMany developers also find creating and maintaining a repository too cumbersome. Canonical tried to make this as easy as possible for PPAs with Launchpad taking care of building and hosting the apps. Nevertheless, the process is inherently more complicated than uploading your app to a store. Although PPAs are still used by many developers, even more software is only available to download from a website.\nThese issues are not unknown to the Flatpak project, which is why Flathub was created: having a single store which contains all apps benefits everyone.\nGiven the downsides of the distributed approach, Canonical decided to invest engineering resources in other parts of Snap instead. Creating a distributed system is a multiplier to complexity; each feature becomes harder to implement. That said, the Snap developers made it clear in the past that if anyone is interested in implementing this feature in Snap, they can definitely do so.\nWill there ever be an alternative Snap store? Let\u0026rsquo;s hope so! Although Snap is designed so that each device only connects to a single store, multiple stores can still exist! A distribution like Manjaro could very well point Snap at their own store. Manjaro chose to use the Snap store hosted by Canonical because they like the advantages that gives them. In the future, however, Manjaro could switch to their own Snap store if that makes sense to them.\nThis is exactly what the Ubuntu Touch project did. Ubuntu Touch is a smartphone OS which uses \u0026ldquo;click\u0026rdquo;, the precursor to Snap. They initially used the click store hosted by Canonical but they created their their own store and switched to it completely when Canonical gave up on their attempt to break the smartphone duopoly.\nIs the Snap Store open source? Sadly, part of the Snap store is still closed source. Snap itself is completely open source and many parts of the Snap store are open source like the web-store front-end, the automatic review tools, the build service, the desktop store app, and many more. The back-end hosting the snaps, however, is still proprietary.\nOpen sourcing the Snap store back-end would require significant changes to it, according to Martin Wimpress of Canonical:\n [because of its history,] the Snap store now integrates with other areas of the Canonical infrastructure. So the Snap store isn\u0026rsquo;t a single thing. It\u0026rsquo;s not like this one piece of software that you can easily decouple from the rest of the machinery that powers the infrastructure at Canonical. So we can\u0026rsquo;t just pull it apart and separate it and say, \u0026ldquo;Here you go, here\u0026rsquo;s the open source Snap store.\n Canonical is doubtful that this investment would be worth it because of what happened with Launchpad. Although they invested significant resources in open sourcing Launchpad, there is still only one instance of Launchpad running and they have not received any significant contributions from non-Canonical employees.\nInterestingly, Canonical actually released an open-source prototype Snap store backend a few years ago, but there was very little interest from the community in in actually maintaining and running a second Snap store, so the project bit-rotted and became incompatible with the current Snap protocol.\nCan other distros curate packages? Linux Mint recently made headlines by blocking users from installing Snap on Linux Mint. One of the reasons they gave for this decision is that the centralization of Snap means Linux Mint cannot provide different versions of certain snaps for their users. They explain that this is possible with APT:\n Thanks to the way APT works, if a bug isn‚Äôt fixed upstream, Debian can fix it with a patch. If Debian doesn‚Äôt, Ubuntu can. If Ubuntu doesn‚Äôt Linux Mint can. If Linux Mint doesn‚Äôt, anyone can, and not only can they fix it, they can distribute it with a PPA.\n It\u0026rsquo;s understandable that a distribution wants some level of control over what packages their users get but this is already possible with the Snap store!\n\u0026ldquo;Brand stores\u0026rdquo; are special copies of the Snap store where admins select which packages from the main store are available and which additional or modified packages are included. The European Space Agency, for example, has a brand store specific to satcom research. The downside of Brand stores is that you\u0026rsquo;re still using the infrastructure of Canonical, but Linux Mint is no stranger to this. Like most Ubuntu derivatives, they heavily relied on Canonical infrastructure when they first started out, even using the regular Ubuntu repositories and mirrors.\nAnother option would be to create their own Snap store with similar functionality and Canonical\u0026rsquo;s previous behavior suggests they would be open to helping that effort.\nMy thoughts about this Having a single Snap store which has closed source parts is very controversial in some parts of the Linux community. The decision to have only a single store per device seems reasonable to me: it has clear advantages both to users and developers. Still, I am happy Flatpak exists as a competing project with a decentralized design and Elementary OS\u0026rsquo;s hybrid approach does a great job of taking advantage of it. I hope that someday the community creates an alternative Snap store similar to what F-Droid is doing.\nI do not like the fact that the Snap store backend is proprietary because it is becoming such a core part of Ubuntu. I don\u0026rsquo;t think there is anything wrong with Canonical producing closed-source add-ons on top of Ubuntu in an \u0026ldquo;Open Core\u0026rdquo; fashion. GitLab does this really well and we have an amazing open source GitHub alternative because of it. There is nothing wrong with Canonical trying to make money; Ubuntu would not be here without them. Linux is great because of all the companies surrounding it.\nWhat I do take issue with is that you currently cannot use Snap for its intended purpose without this proprietary back-end. Ironically, Ubuntu\u0026rsquo;s former community member, Jono Bacon, made it very clear in a recent video that the proprietary parts of an Open Core project need to be optional. Having paid enterprise features such as the Snap Store Proxy is fine, for example, because Snap is still completely functional without it.\nThat said, I\u0026rsquo;m pragmatic about the proprietary back-end. DockerHub and GitHub are insanely popular and they are completely proprietary. Moreover, Canonical is much more receptive to community pressure than Docker inc or GitHub so we are in a much better position to steer them towards doing \u0026ldquo;the right thing\u0026rdquo;. Snap offers a lot of advantages compared to APT and compared to Flatpak, and if things really don\u0026rsquo;t work out with Canonical, we can always build our own back-end.\n Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-08-02-why-one-snap-store/"
  },{
    "title": "Copyleft and LGPL in Snaps and Flatpaks",
    "date": "",
    "description": "",
    "body": "The question often comes up of how Snaps and Flatpaks influence copyleft, GPL and LGPL licenses. It is a common misconception that these new packaging formats significantly influence license compliance.\nDisclaimer: I am not a lawyer, this is not legal advice.\nDistributing your software in a Snap has no effect on GPL copyleft and LGPL compliance.\n It has no effect on copyleft because containers do not trigger copyleft according to the GNU GPL FAQ [see note]. It has no effect on LGPL compliance because the user can modify the LGPL library and relink the application using LD_LIBRARY_PATH or by unsquashing the snap.  If your software is compliant when running outside of a snap, it will still be compliant when running inside of a snap.\nLonger explanation Two questions arise about GPL licensing and snaps.\nWhen my snap includes GPL software, should everything in the snap be released as open source? No.\nWhen you add copyleft code to your application, you need to open source your entire application. The GPL is the most widely-used copyleft license and it uses the term combined work to describe an application which includes GPL code. The entire combined work needs to be open source, even if only a small part is GPL.\nThe GPL clearly makes the distinction between a \u0026ldquo;combined work\u0026rdquo; and an ‚Äúaggregate‚Äù:\n If GPL code is part of a combined work, all parts of that combined work need to be open sourced. If GPL code is \u0026ldquo;aggregated\u0026rdquo; together with other software, the other software does not need to be open sourced.  So the question becomes \u0026ldquo;is a snap a combined work or a mere aggregation?\u0026rdquo;. The GPL FAQ is quite clear that containers, such as Snaps or Flatpaks, do not create combined works [see note]:\n Q: When it comes to determining whether two pieces of software form a single work, does the fact that the code is in one or more containers have any effect?\nA: No, the analysis of whether they are a single work or an aggregate is unchanged by the involvement of containers.\n This works in both ways: if your application is a combined work outside of a container, it will be a combined work inside of a container. If your application is an aggregate outside of a container, it will be so inside of a container. See What is the difference between an ‚Äúaggregate‚Äù and other kinds of ‚Äúmodified versions‚Äù? for more information.\nWhen my snapped application uses an LGPL library, should the application be released as open source? No.\nClosed source application are allowed to link to LGPL libraries as long as a number of requirements are met. The GNU GPL FAQ explains the one most relevant to snaps [see note]:\n For the purpose of complying with the LGPL (any extant version: v2, v2.1 or v3):\n(1) If you statically link against an LGPLed library, you must also provide your application in an object (not necessarily source) format, so that a user has the opportunity to modify the library and relink the application.\n(2) If you dynamically link against an LGPLed library already present on the user\u0026rsquo;s computer, you need not convey the library\u0026rsquo;s source. On the other hand, if you yourself convey the executable LGPLed library along with your application, whether linked with statically or dynamically, you must also convey the library\u0026rsquo;s sources, in one of the ways for which the LGPL provides.\n When you put an application which uses dynamically linked libraries in a snap, the dynamic libraries are distributed in the same snap, but they are not linked yet. When your application starts, the libraries are dynamically linked to your application, just as they would outside of a snap.\nSome might argue that a snap is statically linked because every library is in the same file. However, even in that case, your application is still compliant because the snap provides the application in an object format so that a user has the opportunity to modify the library and relink the application: the individual binaries and libraries of the snap are available in the filesystem at /snap/\u0026lt;snap-name\u0026gt;/\u0026lt;revision/ or by unsquashing the snap manually, and users can relink applications using LD_LIBRARY_PATH.\nNote I use the FAQ as an authoritative source because it holds legal value and the language is a lot clearer than the licenses themselves.\nIn license infringement cases, the spirit of the license holds value. This means what the creators of a license wanted to achieve with it, is important. The GNU GPL FAQ is written by the creators of the GPL license and explains what their intention was. Because of this, the FAQ can be used in court to explain what the license means.\n Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-07-17-lgpl-copyleft-snaps/"
  },{
    "title": "Why Linux desktop apps need containers",
    "date": "",
    "description": "",
    "body": "The Snap Store and Flathub are two universal app stores for Linux. They are very different from how traditional software distribution works. As is always the case with new software, the question \u0026ldquo;why do we need this?\u0026rdquo; often arises. \u0026ldquo;Including software in distribution repositories has worked for so long, so why do we need to change it?\u0026rdquo;\n Note: for an introduction into Flatpak and Snap, please read The future of Linux desktop application delivery is Flatpak and Snap.\n There are many reasons we need Snaps and Flatpaks, but this post will discuss two:\n We need it for software that is not developed at the same speed as distributions. We need it for software that is too \u0026ldquo;messy\u0026rdquo; for distributions.  Let\u0026rsquo;s look at some examples based on snap packages I\u0026rsquo;m maintaining.\nApps which release too quickly Distributions such as Ubuntu and Fedora release a regular version about twice a year. The software available to that version changes very little in between releases, so users often have to wait six months to get major updates to an app. To make matters worse, users of the Ubuntu LTS version have to wait two years for major updates.\nIn contrast, the Arduino IDE sometimes releases a new version after just one month. New versions of the Arduino IDE add support for new hardware and software, so it\u0026rsquo;s important for users to get the latest version as soon as possible. The Arduino IDE Snap updates immediately when a new version is released because the app and its dependencies are completely isolated from the OS itself.\nApps which release too slowly \u0026ldquo;Scratch for Arduino (S4A)\u0026rdquo; gives kids an introduction into robotics by allowing them to program robots using scratch. Although this application is still used for a lot for STEM introductions, it uses very old libraries, making it very hard to install on a recent Linux distribution. The application is developed by a group of educators as a side project and they do not have the resources to update S4A every six months so it works with the newest Ubuntu libraries. The snap of Scratch for Arduino solves this issue by including set of older 32-bit libraries into the snap. These libraries still receive security fixes, but it is very hard to install them on newer Ubuntu releases. Thanks to the snap container, this piece of software can still be used on newer Ubuntu releases.\nApps which are too messy The Bridge Designer is used in middle- and high-schools to give kids a realistic introduction to engineering. The kids get to build, design and test a bridge, similar to how actual engineers do it. The problem with this application is that it is actually a Windows application. Windows software has no place in the Debian repositories. It is however possible to run the software on Linux using a specially crafted Wine environment, which is exactly what the Bridge Designer snap does. Thanks to this snap, users can now install Bridge Designer on Linux without having to mess with Wine.\nMyth: it\u0026rsquo;s only for closed source software All these applications are open source. It is a common myth that these kind of issues only happen in closed source software, but this is clearly not the case. Being open source does not mean a project has the resources to be ported to Linux and it does not mean the developers have enough time to update their software every six months.\nExtra 1: Arduino is also too messy Next to the \u0026ldquo;speed\u0026rdquo; issue, the Arduino IDE has not been updated in Debian and Ubuntu since 2015 because of licensing issues. Arduino source code uses a number of different open source licenses and according to Debian rules, it is not clear enough which files use which license. Although I think this issue should be fixed, Ubuntu users still need the Arduino IDE in the meantime. Because the snap is completely self-contained and does not affect any other part of the OS, it does not need to follow the strict rules of Debian.\nExtra 2: Alternatives Since these issues are universal, people have come up with other very smart solutions.\n AppImage solves these issues by creating a \u0026ldquo;fat binary\u0026rdquo; for each application. Each binary contains all the dependencies an application needs. One advantage is that an AppImage runs by itself; you do not need an additional package manager to run an app. Nix is a package manager which allows multiple installed apps to use different versions of the same libraries. One advantage over Snaps and Flatpaks is that applications generally use less space because shared libraries are not actually duplicated. \u0026ldquo;Rolling\u0026rdquo; distributions such as Manjaro do not release every X months, but continuously releases updates at the same pace of the actual developers of the software. This solves the \u0026ldquo;too fast\u0026rdquo; apps, but not the other issues. This is one of the reasons Manjaro also includes Snapd and Flatpak by default.   Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-07-06-why-snap-flatpak/"
  },{
    "title": "A fundamental difference between Snap and Flatpak",
    "date": "",
    "description": "",
    "body": "Snaps and Flatpaks are often compared to each other because they both make it super easy for Linux users to get the latest versions of desktop applications. If a Linux user wants to install the latest version of apps like Slack, Krita or Blender, either tool will work just fine. There is one fundamental difference between Snaps and Flatpaks, however. While both are systems for distributing Linux apps, snap is also a tool to build Linux Distributions.\n Note: Snap and Flatpak are the software behind two universal Linux app stores: the Snap Store and Flathub. For an introduction, please read The future of Linux desktop application delivery is Flatpak and Snap.\n Flatpak is designed to install and update \u0026ldquo;apps\u0026rdquo;; user-facing software such as video editors, chat programs and more. Your operating system, however, contains a lot more software than apps. It contains a kernel, printer drivers, audio subsystems and more. While Flatpak assumes this software is installed using a traditional package manager, snaps can install anything. These are some examples.\n There is current work ongoing to put the entire Linux printing stack inside of a snap. This has the advantage that printer drivers can be updated independently from the operating system. Once this work is complete, every single Ubuntu version will be able to use the latest printer drivers. Trying to use new printers on old Linux distributions can be very frustrating, and installing newer printer drivers can be risky. Having the printing stack in a snap will solve this issue. A few years ago, Ubuntu drastically changed the system theme. When this initiative started, we wanted an easy way to make the latest updates of the theme available to users immediately. Normally, a system theme is shipped together with the distro, so users do not get get theme updates after the distro releases. For \u0026ldquo;CommuniTheme\u0026rdquo;, however, we fixed this by putting the system theme inside of a snap. Because of this, users got updates to their theme every day, instead of every 6 months. This is again not something Flatpak was built for. Flatpak applications can update their own theme, but it is not possible to ship the system theme as a Flatpak. This is because Flatpak was designed for distributing apps, not building an entire Linux distribution. Even the Linux kernel, the most fundamental part of a Linux distribution, can be put in a snap. This is used a lot for IoT devices such as routers and satellites. The impact of a broken kernel update is catastrophic if you require a rocket in order to plug a USB stick into the device. Snaps allow these devices to safely update their kernel and automatically roll back if something goes wrong during the process.  As a result, it\u0026rsquo;s possible to build an entire operating system using only snaps, which is exactly what Ubuntu Core is.\nJust to be clear, this is not meant to be criticism of Flatpak. Flatpak was designed to give developers an easy way to bring their apps directly to users, and it does that job very well. The focussed approach of Flatpak even has a big advantage: it\u0026rsquo;s a lot easier for a distribution to integrate with Flatpak because it does a lot less. The tradeoff is that it only provides app distribution; it doesn\u0026rsquo;t solve the issues of distributing entire operating systems. Fedora Silverblue, for example, creates an immutable desktop operating system by using Flatpak for app distribution and OSTree for distributing the OS itself.\n Note: I am a human being, and like most human beings, I make mistakes. Did you find an issue with this article? Let me know in the comments, and I\u0026rsquo;ll be happy to correct it!\n ",
    "ref": "/blog/2020-07-03-snap-vs-flatpak/"
  },{
    "title": "Tutorial: S4A on Ubuntu",
    "date": "",
    "description": "",
    "body": "S4A, or Scratch for Arduino, is an app that you can use to program Arduino boards using Scratch. You can program robots and LED\u0026rsquo;s without writing any code: all you need to do is drag and drop instructions in the visual programming environment.\nStep 1: Install Arduino IDE and S4A The first step is to install Arduino and S4A from the Snap Store.\nsudo snap install arduino s4a After that, you need to give the user account on your computer access to USB devices. Only administrators and user in the dialout group have access to Arduino board connected over USB. Because of that, you will get a \u0026ldquo;Permission Denied\u0026rdquo; error when you try to upload a program. In order to give your user access, run the following command and restart your computer.\n# Add the current user to the `dailout` group sudo usermod -a -G dialout $USER After this, you need to restart your computer.\nStep 2: Upload S4A firmware to Arduino board The next step is to Flash the S4A firmware on your Arduino board. This is required so that S4A can connect to your board to run your apps on it. You need to install this firmware using the Arduino IDE.\n  Download the S4A firmware from here and open it in the Arduino IDE.\n  Connect your arduino UNO board to your computer using USB.\n  Upload the firmware to your arduino by clicking the right arrow at the top of the Arduino IDE. When the firmware is uploaded you\u0026rsquo;ll see the text \u0026ldquo;Done Uploading.\u0026rdquo; on the grey bar just below the code.\n  Step 3: Use S4A After the upload succeeds, you can open the S4A program and start programming your board!\nFrequently Asked Questions When I upload code to the Arduino, I get a \u0026ldquo;permission denied\u0026rdquo; error When you get the following error, that means that the Arduino IDE is not allowed access to the Arduino board.\nAn error occurred while uploading the sketch avrdude: ser_open(): can't open device \u0026quot;/dev/ttyUSB0\u0026quot;: Permission denied This means that either the IDE or your current user is not allowed access to the Arduino USB device. Make sure you execute the following commands as the user which runs the IDE.\n# Give the current user access to the USB device. sudo usermod -a -G dialout $USER After this, restart your computer and try again.\n Note: if the user which runs the IDE does not have sudo permissions, you can grant the user permissions from another account by running sudo usermod -a -G dialout \u0026lt;username\u0026gt; and replacing \u0026lt;username\u0026gt; to the name of the user running the Arduino IDE.\n ",
    "ref": "/blog/2020-02-s4a-setup-ubuntu/"
  },{
    "title": "What's up with CRI-O, Kata Containers and Podman?",
    "date": "",
    "description": "",
    "body": "The container ecosystem is moving rapidly. A lot happened in 2019! It seems the container ecosystem has arrived at the \u0026ldquo;consolidation\u0026rdquo; stage of the hype cycle:\n Docker Inc. sold off its enterprise business and had a bunch of restructuring. They shelved Docker Swarm saying \u0026ldquo;The primary orchestrator going forward is Kubernetes.\u0026rdquo; Twitter joined Spotify in moving away from Mesos-based container orchestration to Kubernetes. This was a huge blow to Mesos given Twitter was its original developer. The Cloud Native Compute Foundation archived rkt. Canonical doubled down on Kubernetes with MicroKubernetes for local development and Charmed Kubernetes for production setups and introduced support for Kata Containers to improve the security and isolation of containers. Red Hat released RHEL8 without Docker support, pointing customers towards their own Podman and CRI-O project.  Kubernetes is the clear winner here, but how do Podman, CRI-O and Kata Containers relate to this ecosystem? The diagram below gives a simplified overview.\nMore explanation below.\nStandards and organizations  The OCI or Open Containers Initiative is an organization that creates container standards. The OCI runtime spec defines the API of a low-level container runtime and the OCI image spec defines what a \u0026ldquo;Docker image\u0026rdquo; actually is. The CNCF is the Cloud Native Compute Foundation, an organization created by the Linux Foundation which hosts a big number of open source projects that all have to do with running \u0026ldquo;cloud native\u0026rdquo; applications. This is mostly about microservices and containers. The Kubernetes project has also defined a number of standards. Relevant for this article is the CRI: the Container Runtime Interface. This interface defines how Kubernetes talks with a high-level container runtime.  But what is a high-level container runtime exactly? First; we\u0026rsquo;ll look at what a low-level container runtime does.\nHigh- vs Low-level container runtime An OCI runtime is relatively simple. You give it the root filesystem of the container and a json file describing core properties of the container, and the runtime spins up the container and connects it to an existing network using a pre-start hook.\nSo what an OCI runtime does not do is the following.\n Actually creating the network of a container. Managing container images. Preparing the environment of a container. Managing local/persistent storage.  All this is the job of a high-level container runtime. On top of this, the high-level container runtime implements the CRI so that Kubernetes has an easy way to drive the runtime.\nAt the moment, we have three main OCI runtimes or low-level container runtimes.\n runc, which is the default for most tools such as Docker and Podman. This is based on the code initially donated by Docker. kata-run from the \u0026ldquo;Kata Containers\u0026rdquo; project, which aims to provide much better security and isolation between containers by running each container in a lightweight VM. It\u0026rsquo;s a merge of the runv and Intel Clear Containers projects. gVisor is created by Google. It provides better isolation by running each container in a tight security sandbox.  There are also three main high-level container runtimes.\n containerd is a CRI-compatible container runtime which was donated to the CNCF by Docker. It is currently the default in many Kubernetes distributions such ad Canonical\u0026rsquo;s Charmed Kubernetes. It supports all OCI-compliant runtimes and has a special shim for kata-run. CRI-O is a bridge between Kubernetes and OCI-compliant runtimes created by Red Hat. It has the big advantage that it gets released in lock-step with Kubernetes itself. Each CRI-O version is compatible with the Kubernetes version that has the same version number. This runtime is the default in OpenShift. Docker itself can also be used as a CIR-compatible container runtime using the docker-shim. However, many Kubernetes distributors are moving away from this solution, due to the added unnecessary complexity of Docker.  Podman, Buildah and crictl Now, what was this about Red Hat moving from Docker to Podman in RHEL8? Scott McCarty wrote an excellent blog series excplaining Why There Is No Docker in OpenShift 4 and RHEL 8. In short:\n Docker is a big fat daemon running almost always as root and which does a million things in the same binary. He argues it\u0026rsquo;s better to split up functionality in a bunch of different projects so you can pick and choose which ones you actually need. Development of the Docker engine is too slow to keep up with Kubernetes. Docker\u0026rsquo;s community is on life support because of how Docker handled the CE/EE split and the move/rename to the Moby project. Docker Inc.\u0026rsquo;s future is uncertain.  So, what should we use instead of Docker?\n Use Podman managing pods and containers. It\u0026rsquo;s a CLI tool which is very similar to docker. It uses libpod which uses runc in backend and is fully compatible with \u0026ldquo;Docker Images\u0026rdquo;. Use Buildah for building \u0026ldquo;Docker Images\u0026rdquo;. It supports building containers from DockerFiles, but you can also build them with simple shell scripts! Use CRI-O for running containers with Kubernetes. If you want to debug pods and containers maintained by Kubernetes, you can use the crictl tool instead of the docker commands.  Conclusion Developers who use Docker and Kubernetes might have been surprised that Docker Inc. is in such bad shape. Further inspection, however, shows an entire ecosystem that is moving on. Docker clearly started this entire revolution, but it just didn\u0026rsquo;t work out. The restructured Docker inc. will focus solely on developers. This might turn out to be an excellent move, since the docker client is still the prime way for developers to build run \u0026ldquo;Docker containers\u0026rdquo; on their development workstations. It will be interesting to watch this space in 2020 and see if Docker is able to implement this vision.\n A special thanks to Kunal Kushwaha for his excellent presentation on Kubecon \u0026ldquo;How Container Runtimes matter in Kubernetes?\u0026quot;. Container ship image by Sergii ILyushin  ",
    "ref": "/blog/2020-01-docker-podman-kata-cri-o/"
  },{
    "title": "How I became a cyborg pt1",
    "date": "",
    "description": "",
    "body": "Close Encounters of the Third Kind \u0026ldquo;We really need to do something about the valve. We\u0026rsquo;ve passed the point where we can wait two more years to see how it progresses.\u0026rdquo;\nI was silent for a while, so the cardiologist asked \u0026ldquo;do you understand what I mean?\u0026rdquo;\n\u0026ldquo;Like, I need a transplant from a pig heart?\u0026rdquo;, I asked.\n\u0026ldquo;Well, given your age an artificial valve is more likely.\u0026rdquo;\nIt was clear my heart was fucked. I knew it wasn\u0026rsquo;t a good sign when the doctor sounded more and more worried as she looked at my heart, and hurried in colleagues to get a second and third opinion. Words like \u0026ldquo;bulging\u0026rdquo; and \u0026ldquo;hypertrophy\u0026rdquo; didn\u0026rsquo;t mean much to to me and I couldn\u0026rsquo;t really ask for clarification since they were looking at my heart through a gigantic tube stuck down my throat. After the procedure, the doctor didn\u0026rsquo;t want to discuss the results with me, saying that the cardiologist would explain everything. She just assured me that \u0026ldquo;everything will work out fine\u0026rdquo;; implying that things were, in fact, not fine.\nThe cardiologist revealed the severity of the issue. Normal hearts have a bunch of valves that make sure blood can only flow in one direction. The aorta valve specifically closes off your heart after each heartbeat to make sure the blood flows to your organs instead of back into your heart. My aorta valve, however, missed that memo. It leaked. Like a lot. Probably a birth defect. My heart took this like a boss for most of my life and just compensated the fuck out of it, but it was starting to take its toll; my heart was becoming enlarged.\n  Frontal image of the bad valve. This should normally look like a Mercedes logo; showing the three leaflets of the valve. However, only the top-left leaflet of this valve is regular. The two others are fused together and badly damaged.    Side view of the valve when it\u0026#39;s closed with my heart on the left and the aorta on the right. The brightly colored stream on the left is blood going the wrong way; it flows back in to my heart, leaking through the closed valve.  Despite this issue, the cardiologist was also optimistic.\n\u0026ldquo;The good news is that, because we found it so early, your heart should be able to recover after we fix the valve. However, really need to do something about it in time. What do you have planned the following weeks?\u0026rdquo;\nI have to admit that 95% of my brain was focused on how his definition of \u0026ldquo;in time\u0026rdquo; was wildly different from mine. \u0026ldquo;umm, I have some time available, I think..\u0026rdquo;\n\u0026ldquo;Great! You can discuss the details with the secretary.\u0026rdquo;\nBefore I knew it, the secretary was calling different departments to schedule a bunch of preparatory exams. Open Heart surgery is pretty gruelling to your body so they wanted to make sure I didn\u0026rsquo;t have any other issues. There was a lot that needed to be done and my brain jumped on it like any other project. Emotionally detached, as if it was a complicated process to fix my computer. I scheduled the appointments, read up on the illness and the procedures and informed family and friends.\nThere was still some uncertainty about the extent of the issue. The issue with my valve was congenital; it just didn\u0026rsquo;t get the memo on how to do this \u0026ldquo;valve\u0026rdquo; thing properly and the doctors wanted to see if any other parts of my heart and veins were slacking off. They did this using a coronarography, or \u0026ldquo;an alien\u0026rsquo;s wet dream\u0026rdquo;, as I like to call it. They opened two veins in my groin, put in long probes to reach my heart, injected a bunch of contrast fluid, and watched it flow using a Rontgen scan. To top it all off, this procedure was done while I was awake and I could follow along on a giant screen next to me. If this sound uncomfortable, that\u0026rsquo;s probably because it is. Thank god I live in Belgium; just imagine living in the US and becoming financially ruined while some doctors reenact the final scene of \u0026ldquo;the fourth kind\u0026rdquo; on you..\nThankfully, the result of the coronarography was positive: the other parts of my heart were nicely executing their assigned tasks, and the surgeons were once again paid to probe a fellow human being. Even better, I now had a bunch of cool videos showing how my heart (mal)functioned.\n  --  Your browser does not support the video tag.  This video shows a probe injecting contrast fluid in my aorta. Most of the fluid is pumped up through the aorta to the rest of the body. However, due to the leaking valve, part of the fluid is sucked down, back into the left ventricle of the heart itself.\n Next step: the heart surgery itself! We scheduled it about six weeks after the coronarography so that I had some time to finish up at work and go on a holiday. However, some complications foiled the plan of finishing up at work. All the probing in my groin tore one of my veins, causing a pseudo-aneurism. After a week of trying to fix it from the outside, the doctors gave up and did a surgery under full anesthesia to sew everything back together. Long story short; I ended up losing four of those six weeks to recovering from the coronarography. Thankfully, the doctor cleared me to go on a holiday after that.\nHow I became a cyborg pt2\n",
    "ref": "/blog/2019-steampunk-cyborg-1/"
  },{
    "title": "Who owns my research?",
    "date": "",
    "description": "",
    "body": "When I submitted my first paper, I realized I had no idea who owned my research! Was I authorized to transfer the rights to a journal or did I need someone else\u0026rsquo;s permission? After some digging, I found the answer and wrote it all up in a neat little document. So here\u0026rsquo;s the dusted off version, in case it\u0026rsquo;s useful to anyone else.\nIn short Intellectual property (IP) as a PhD student at Ghent University is largely divided into two parts: research results and educational material.\n The IP on research results are automatically transferred to the university, except for literary and artistic works. So you own the papers you write and the university owns the patents you create. The software and databases you create are also owned by the university. This is an exception to the general rule since Software is generally seen as a literary or artistic work. The IP on the educational material you create is completely owned by you, except for software and databases. So the course materials and educational books you write are completely owned by you. As a result, your students need to have your permission to copy and sell your course materials.  a) Research Results Note that different rules might apply if your research is in collaboration with a company or another research institution.\nGhent University Education and Examination Code Article 85 of the Ghent University Education and Examination Code deals with IP on research results.\n ¬ß1. In execution of article 4 of the General Research and Co-operation Regulations of Ghent University Association (Algemeen Onderzoeks- en Samenwerkingsreglement van de Associatie Universiteit Gent, AOSR), all doctoral students who are considered voluntary researchers in accordance with article IV.48 of the Codex Higher Education transfer all property rights of their research results to Ghent University upon enrolment. Unless stated otherwise in their contract, all doctoral students are treated as researchers at Ghent University as far as the implementation is concerned of all applicable regulations on the valorization of research results.\n¬ß2. The supervisor(s) and the doctoral students see to it that all research results that can create value are reported to the Technology Transfer Office prior to publication in any which shape or form, in accordance with the AOSR.\n The document also explains what \u0026ldquo;research results\u0026rdquo; are.\n The results of research or development efforts, accomplished by the researchers as part of their relation with Ghent University and/or by means of Ghent University resources or equipment. These do not comprise literary works or works of art as intended under the Law on Copyright and Neighbouring Rights of 30 June 1994 (‚ÄòCopyright Act‚Äô). However, they are considered to comprise computer software or databases which are protected under the Copyright Act and/or the Act of 31 August 1998 which transposed into Belgian law the European Directive of 11 March 1996 on the Legal Protection of Databases (‚ÄòDatabase Act‚Äô).\n Higher Education Codex The Higher Education Codex is a Flemish decree governing the higher education in Flanders. It is sadly not available in English, so all translations here are of my own.\nArticle IV.48.. explains that the university own the IP rights on the research results of its personnel and researchers associated with the university. \u0026ldquo;Research results\u0026rdquo; is defined as potentially patentable inventions, (biological) culture products, drawings and models, semiconductor topographies, computer programs and databases that are useable for industrial or agricultural industrial purposes.\nNote that this excludes literary works.\nUgent Valorisation Regulation The Ghent University Valorisation Regulation also talks about the IP of research results. It is sadly also not available in English. It defines what \u0026ldquo;research results\u0026rdquo; as \u0026ldquo;results of research and development that researchers create as part of their relationship with Ghent University and/or using resources of Ghent University\u0026rdquo;. It explicitly says software and databases are part of research results, but literary works and works of art are not.\n De Onderzoeksresultaten zijn de resultaten van onderzoek of ontwikkeling die door de Onderzoekers worden gerealiseerd in het kader van hun relatie met de Universiteit en/of dankzij het gebruik van de universitaire middelen of uitrusting. Werken van letterkunde of kunst in de zin van Boek XI, Titel 5 van het Wetboek van Economisch Recht (de ‚Äúauteurswet‚Äù) maken voor toepassing van onderhavig reglement geen deel uit van de Onderzoeksresultaten. Computerprogramma\u0026rsquo;s of databanken beschermd krachtens Boek XI, Titel 6 en 7 van het Wetboek van Economisch Recht maken wel deel uit van de Onderzoeksresultaten. Onderzoeksresultaten behaald door een student, bijvoorbeeld in het kader van een afstudeerwerk, behoren ook tot de Onderzoeksresultaten zoals hier gedefinieerd in de mate dat bij de creatie van resultaten van de student beroep gedaan wordt op universitaire middelen.\n\u0026hellip;\nARTIKEL 2 VERMOGENSRECHTEN OP ONDERZOEKSRESULTATEN 2.1 De vermogensrechten op de Onderzoeksresultaten komen toe aan de Universiteit Gent als rechtspersoon, behoudens andersluidende bepalingen in reglementeringen en/of overeenkomsten goedgekeurd door het Universiteitsbestuur. 2.2 De Universiteit eerbiedigt de morele rechten, inclusief het vaderschapsrecht, die uitsluitend eigendom blijven van de Onderzoekers.\n imec Confidentiality Agreement Researchers at the IDLab research group at Ghent University were asked to personally sign a confidentiality agreement with imec. Since this agreement itself is not publicly available, I will not talk about it further, but you should read it if it applies to you.\nb) Educational Materials Neither the Ghent University Education and Examination Code nor the Ghent University Valorisation Regulation talk about IP rights on educational materials created by employees. As a result, the belgian copyright law applies here.\nThe Belgian copyright law states that creators of a work own the IP of that work by default, even if it is made as part of their job. The law goes on to say that employment contracts can state that copyright is automatically assigned to the employer in certain conditions.\n Auteursrechten ontstaan bij de maker van een werk. Het zal dus de arbeider, bediende of personeelslid onder statuut zijn, en niet de werkgever, die de rechten verwerft op een werk dat door hem is gemaakt in het kader van een arbeidsbetrekking. Deze regel is daarentegen niet van toepassing voor computerprogramma‚Äôs, databanken en tekeningen en modellen.\n\u0026hellip;\nEen overeenkomst tot overdracht van auteursrechten, zoals hierboven beschreven, zal slechts geldig zijn met betrekking tot de werken die de werknemer heeft gemaakt in het kader van zijn arbeidsovereenkomst of de hem toevertrouwde opdracht.\n The valorisation department of Ghent University confirmed that this is the case; the course materials you create are owned by you.\nIf you contact the department of valorisation of Ghent University, they will confirm that this is the case: the course materials you create are owned by you.\nMaak je een cursus in opdracht van de school, dan wordt de school automatisch eigenaar en heeft er dus auteursrechten over. Ontwerp je op eigen initiatief een cursus, dan heb jij de auteursrechten. * [Source](https://onderwijs.vlaanderen.be/nl/auteursrechten-op-school#Zelfgemaaktecursus) * [Source](http://anspire.be/auteursrechten/download/Lespakket%20auteursrechten%20en%20licenties%20-%20v2.pdf) -- ",
    "ref": "/blog/copyright-ghent-university-researcher/"
  },{
    "title": "I'm Back!",
    "date": "",
    "description": "",
    "body": "I restored my blog from an old backup I had. I imported it into Hugo, a static blogging platform.\nImage by clipart-library\n",
    "ref": "/blog/2019-05-im-back/"
  },{
    "title": "What does ‚Äúperf interrupt took too long‚Äù mean?",
    "date": "",
    "description": "",
    "body": "Wifi problems So, I\u0026rsquo;m having a problem with my laptop. When I\u0026rsquo;m using the ‚ÄúTelenetWiFree‚Äù connection, I get disconnected after a certain amount of time, and for some reason I cannot reconnect until I restart my computer. Toggling the hardware Wifi¬†kill-switch, which lets you disable and enable the power to the wifi hardware, does not resolve the problem. After re-enabling the hardware, the wifi doesn\u0026rsquo;t seem to come back again. Only a reboot makes the Wifi work again‚Ä¶\nThe ‚ÄúTelenetWiFree‚Äù Wifi doesn\u0026rsquo;t play well with all the computers we have, three Ubuntu laptops and one Chromebook, but it seems particularly wonky with my computer. I had some free time, so I started digging into the problem.\nWho watches the watchmen? It seems the kernel does‚Ä¶ In the dmesg output, I found the following line:\n perf interrupt took too long (2528 \u0026gt; 2500), lowering kernel.perf_event_max_sample_rate to 50000\n This doesn\u0026rsquo;t seem right, maybe this is the source of the problem! Let\u0026rsquo;s do some digging:\n Perf¬†is a Linux Kernel performance monitor. Perf uses Non-Maskable Interrupts. This basically means Perf can tell the cpu¬†‚ÄúPauze whatever you\u0026rsquo;re doing now and let me do something‚Äù. This allows perf to have a ‚Äúwatchdog‚Äù functionality where it can monitor even the most critical processes and interrupts.  With great power comes great responsibility. Perf can hang your computer by constantly pauzing everything. To make sure this won\u0026rsquo;t happen, the kernel itself monitors perf. When it decides perf is pauzing too long, it tells perf to do a little bit less. That\u0026rsquo;s basically what that dmesg line means. The kernel told perf to do a little bit less.¬†Does it have anything to do with the Wifi problem? Probably not‚Ä¶ But at least I learned something: The kernel watches the watchmen‚Ä¶\nAdditional sources:\n https://superuser.com/questions/757444/perf-samples-too-long-lowering-kernel-perf-event-max-sample-rate-to https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1246216 https://bbs.archlinux.org/viewtopic.php?id=170471 http://ubuntuforums.org/showthread.php?t=2253552  Image by David Masters\n",
    "ref": "/blog/2015-09-perf-interrupt-took-to-long/"
  },{
    "title": "Pragmatic Docker Day 2015",
    "date": "",
    "description": "",
    "body": "I got a ticket to the Pragmatic Docker Day meetup in Ghent in exchange for writing a blogpost about it. Free food, drinks \u0026amp; awesome talks for a whole day, who would want to miss that?¬†Not me!\nSo here are my unstructured thoughts I gathered from the meetup:\nDocker is awesome! What is docker? Docker encapsulates apps in their own little sandbox. Every app runs in its own container. An environment made especially so the app can run well and can\u0026rsquo;t mess to much with other apps running on the same server. It\u0026rsquo;s basically a VM without the overhead.\n Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications\n Developers write their apps and put them in a docker container. They give this container to sysadmins and they are sure everything will work once deployed on the server because the app and all its dependencies are bundled in the docker container.\nDocker is fancy! This isn\u0026rsquo;t new at all. Containers have existed on Linux for a very long time. What docker does is provide a very fancy interface and API to work with and distribute those containers. This opens container technologies to a much broader public.\nDocker is becoming so popular that even Microsoft wants in on the party. Docker announced a partnership with Microsoft to bring Docker to Windows.\nDocker is production-ready* Tomas Doran talked about how Yelp is using Docker in their Continues Integration \u0026amp; Continues Deployment workflow. From what I gathered, almost every service at Yelp is running in docker containers. This allows for very dynamic scaling, and makes the development ‚Äì testing ‚Äì production chain a lot shorter without compromising on the stability.\n*Docker is still a very young and fast-moving platform. Using this in production requires a lot of knowledge and requires some caution. For a small infrastructure the benefits you get from docker might not outweigh the disadvantages from working with such early technology. However, even when you don\u0026rsquo;t use Docker in production, you can still benefit from it during development and testing!\nDocker containers are not cross-platform‚Ä¶ The docker container still uses the host\u0026rsquo;s kernel. This means that a docker container will only work on a machine that supports the same kernel functionality. If you make a docker container on Linux, and you run it on a distro with a different version of the Linux kernel, most of the time you\u0026rsquo;ll be allright.¬†However, running Linux docker containers native on a Mac or Windows won\u0026rsquo;t work.\nYou can still run containers on Windows?! It\u0026rsquo;s still possible to run Linux docker containers on Mac and Windows using boot2docker. This is basically a lightweight Linux VM. Docker containers run in this VM. This is great for development, but there might be an overhead because of the virtualization.\nImage by distel2610\n",
    "ref": "/blog/2015-04-pragmatic-docker-day/"
  },{
    "title": "Windows power users are not Linux power users!",
    "date": "",
    "description": "",
    "body": "Let\u0026rsquo;s rid the world of one of the most prominent Linux myths:¬†‚ÄúDesktop Linux sucks. It\u0026rsquo;s just not there yet‚Äù.\nWhen Windows power users try out Linux two things always happen:\n They get frustrated because they can\u0026rsquo;t find how to do something in Linux. They brick their system, or make it extremely buggy.  I\u0026rsquo;ve been that person, I\u0026rsquo;ve bricked my system (a lot!), and the fact is, it was entirely my own¬†fault. Linux didn\u0026rsquo;t¬†suck, I made it suck! This post is a summary of every mistake I made the last 4 years, in an attempt to prevent other people from making the same ones.\n  Let\u0026#39;s rid the world of a few Linux misconceptions  Linux is different Let\u0026rsquo;s start with¬†an example: Bill¬†is trying out Ubuntu and wants to change his screen resolution. Bill¬†knows that on Windows, you can right-click the desktop background and click ‚Äúchange screen resolution‚Äù. Bill¬†tries this, but cannot find the ‚Äúchange screen resolution‚Äù option. Bill¬†can now do two things:\n [the bad way] Assume you can only change this using the commandline. Go online and rant about how Ubuntu is impossibly hard to use. [the good way] Open the dash and search for ‚ÄúResolution‚Äù or ‚Äúscreen‚Äù or ‚Äúdisplay‚Äù or ‚Äúmonitor‚Äù or ‚Äúprojector‚Äù¬†or whatever. He will find the ‚ÄúDisplays‚Äù application. If he isn\u0026rsquo;t entirely sure if that is the right application, he can right-click the icon and he will see a description of what the application does.  Linux is not Windows. You are used to doing things a certain way on Windows. Some things will work differently on Linux. You will have to get used to it. This does not mean Linux is hard, only that it is different. Mac has this exact same problem. Windows power users complaining that ctrl-c ctrl-v does not work on a mac, even though the command button makes a lot of sense.\n  And yes, you can change the resolution in a GUI  Windows power user != Linux power user Another example: Jessica knows a thing or two about Windows, she can even re-install Windows, if she finds that dvd she once burned.¬†Jessica want to try out Ubuntu. She takes an old computer lying around and starts up the Ubuntu installer. She knows it\u0026rsquo;s better to have different partitions, so she chooses to partition the disks manually. She tries to configure the partitions, but she keeps getting errors she does not understand. She gets frustrated and rants on G+ about how¬†in Linux, everything is¬†complicated.\nI see this mentality a lot: ‚ÄúI know how to do advanced¬†tasks on Windows. I don\u0026rsquo;t know how to do advanced¬†tasks on Linux, so Linux must be hard.‚Äù\nYou can do advanced tasks on Windows, because you learned how to do them on Windows. You will have to learn how to do some of them for Linux.¬†This is the same for every platform. You can be a complete Linux Guru, being able to install Linux From Scratch.¬†But you will still have to relearn how to flash your android phone.\nGive it time, after a while you will come to have the same skill level on Linux as you have on Windows. Just don\u0026rsquo;t expect to get there on¬†the first day.\nYou are a danger to your Linux Exhibit A: Alice her Windows machine is becoming really slow and she wonders what she could do to speed it up a little. Alice googles ‚Äúhow to speed up Windows‚Äù. One of the first results is a blog explaining how to use cCleaner to disable processes.¬†Alice, being a Windows power user, knows this can break her system really bad, so she is very careful and googles each process before disabling it. Alice is happy, her Windows is faster again.\nExhibit B: Bob is running the latest Ubuntu on a 10-year old laptop and notices it is a bit slow. Bob is a Windows power user, he figures he knows enough about computers to do something about it. He googles ‚Äúhow to make Ubuntu faster‚Äù. He finds a blog¬†telling him to run different commands. The blogpost¬†is a year old and has a lot of ‚Äúthanks!‚Äù comments, so Bob thinks he can trust the author. Bob runs the commands. When a command fails, he figures it is a permission problem, so he runs the command again with ‚Äúsudo‚Äù. The following week, Bob experiences weird glitches and crashes, and his computer cannot connect to his printer anymore. Bob is unhappy and goes on twitter to rage about how buggy Linux is.\nYou\u0026rsquo;re a Windows power user. You can mess with Windows, because you know what is dangerous, you know what warnings you can safely ignore. You are not a Linux power user. You do not know how to make that distinction on Linux, so be very careful!\n  Do not blindly trust commands from the web  I cannot stress this enough. I\u0026rsquo;ve seen this happen so many times, with myself, and with other people. A friend of mine wanted to make¬†a lightweight Ubuntu install for a media center. He was using a heavily outdated guide to do so. The guide instructed him to remove a lot of programs, including compiz. Little did he know that newer versions of Ubuntu(Unity) require compiz to function properly. The result: he bricked his system, and blamed Ubuntu in the process.\nA¬†Desktop environment is not a theme A desktop environment(DE) is a collection of software that does a lot more than just ‚Äúlook good‚Äù. A DE handles a lot of the ‚Äúusability‚Äù features of the desktop, like automounting USB-sticks, CD\u0026rsquo;s, DVD\u0026rsquo;s and memory cards. It also helps you set up your network, it configures DHCP, detects wireless networks and gives you a nice user interface to enter the WIFI password. A DE also handles the function keys (brightness, sound, ‚Ä¶) on your keyboard, and it can even connect to your smartphone to show you if you have new messages.\nWhen you choose a DE, you basically choose how you will interact with your computer.¬†If you choose a lightweight DE like lxde, you will lose a lot of the out-of-the-box experience a Windows/Mac user might expect. You will have to pop open a terminal, even to do basic stuff, like change your timezone. Unity on the other hand has those features, but demands more from your hardware.\n  Xubuntu might be fast and stable, but it comes with a price  A lot of the DE\u0026rsquo;s share¬†the same libraries and software. Installing multiple DE\u0026rsquo;s at the same time can cause problems. If you want to try out different DE\u0026rsquo;s, I recommend doing a clean install with one of the official Ubuntu flavours, or another distro.\nChoose wisely, and stick to that choice Linux gives you a lot of choice. This is a great strength, but also a weakness. People have a tendency of making bad decisions when presented with so¬†much choice.\nEveryone who tried Linux for the first time has had the same question: ‚Äúwhat distro should I use?‚Äù. This¬†seems like a very hard question, and the internet gives you a lot of conflicting answers to it.¬†However, for 99% of the people, the answer is really simple.\nIf you\u0026rsquo;re looking for a Window/Mac replacement for your primary machine, and you are new to Linux,¬†you should use default Ubuntu. Ubuntu offers the complete out-of-the-box experience you are used to on Windows/Mac. It is the best supported desktop, and It also has a good community that is very newbie-friendly.¬†Every problem you will have, someone has had already. Askubuntu is¬†full of¬†questions Windows and Mac users might have when switching to Ubuntu.\n  Askubuntu, helping Windows users change to Linux since 2010  You will surely find a DE that is easier to use than Unity. You will also find one that is more stable and one that is more beautiful. However,¬†Unity offers the complete package with very few rough edges.\nIt\u0026rsquo;s also easy to find a¬†Distro that is more up-to-date than Ubuntu. Finding one that is¬†more stable and one that is faster is easy too. But again, Ubuntu offers the complete package, with very few rough edges. It makes the transition very easy and it is a care-free Windows/Mac replacement.\n  With a Numix theme, Unity looks pretty good!  Conclusion Are you thinking of joining the cool kids and trying out Linux? Then start with an easy distro¬†like Ubuntu and be careful with what you do in the commandline. Most important of all,¬†remember that Linux is different, and that\u0026rsquo;s a good thing.\nEnjoy your Linux, and let me know your experiences in the comments down below.\n",
    "ref": "/blog/2014-11-why-you-think-linux-sucks-and-why-its-your-own-fault/"
  }]
